{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# our libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Create a model, m, with the line   m = read_word2vec_model()\n",
    "#\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "def read_word2vec_model(filename = \"word2vec_model.txt\"):  \n",
    "    \"\"\" a function that reads a word2vec model from the file\n",
    "        \"word2vec_model.txt\" and returns a model object that\n",
    "        we will usually name m or model...\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(\"Starting to load the model in \", filename, \"...\")\n",
    "        model = KeyedVectors.load_word2vec_format(filename, binary=False)\n",
    "        print(\"Model loaded.\\n\")\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"  [WARNING]    The file {filename} was not found.     [WARNING]  \")\n",
    "        return None   # returning a placeholder, not a model\n",
    "\n",
    "    # let's print some attributes\n",
    "    print(\"The model built is\", model, \"\\n\")\n",
    "    print(\"The vocabulary has\", model.vectors.shape[0], \"words\")   # The vocabulary has 43981 words\n",
    "    print(\"Each word is a vector of size\", model.vector_size)  # 300\n",
    "    print(\"\\nTry m.get_vector('python') to see a the vector for 'python'!\\n\")\n",
    "    model.fill_norms()  # freezes the model, m, as-is (no more training)\n",
    "    # we weren't going to train more, so no worries (in week7, at least)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to load the model in  word2vec_model.txt ...\n",
      "Model loaded.\n",
      "\n",
      "The model built is KeyedVectors<vector_size=300, 43981 keys> \n",
      "\n",
      "The vocabulary has 43981 words\n",
      "Each word is a vector of size 300\n",
      "\n",
      "Try m.get_vector('python') to see a the vector for 'python'!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "# best to run this only once... or once in a while\n",
    "#\n",
    "m = read_word2vec_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### More word-embedding geometry:  Analogies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('snake', 0.660629153251648),\n",
       " ('crocodile', 0.6591362953186035),\n",
       " ('alligator', 0.6421656012535095),\n",
       " ('boa', 0.5617719888687134),\n",
       " ('constrictor', 0.5378887057304382),\n",
       " ('constrictors', 0.5356365442276001),\n",
       " ('snakes', 0.5345131754875183),\n",
       " ('anaconda', 0.5207394361495972),\n",
       " ('rabbit', 0.5074971318244934),\n",
       " ('tortoise', 0.5046288967132568)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "# Let's take a look at some additional \"geometry\" of word-meanings (cool!)\n",
    "#\n",
    "\n",
    "m.most_similar(positive='python', topn=10)  # negative='snake'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Java', 0.22111035883426666),\n",
       " ('Dior', 0.2102828025817871),\n",
       " ('Notte', 0.207855224609375),\n",
       " ('os', 0.19944755733013153),\n",
       " ('frock', 0.19739560782909393),\n",
       " ('blouse', 0.19704443216323853),\n",
       " ('plaids', 0.19696445763111115),\n",
       " ('blazer', 0.1878664493560791),\n",
       " ('gown', 0.17895956337451935),\n",
       " ('Gala', 0.17834939062595367)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "# With this most_similar method, we can \"subtract\" vectors, too:\n",
    "#\n",
    "\n",
    "m.most_similar(positive='python', negative='snake', topn=10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('queen', 0.7118193507194519),\n",
       " ('monarch', 0.6189674139022827),\n",
       " ('princess', 0.5902430415153503),\n",
       " ('prince', 0.5377321243286133),\n",
       " ('kings', 0.5236843228340149),\n",
       " ('queens', 0.5181134939193726),\n",
       " ('throne', 0.5005807280540466),\n",
       " ('royal', 0.493820458650589),\n",
       " ('ruler', 0.49092739820480347),\n",
       " ('princes', 0.481081485748291)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "# Here, see if you can determine the analogy that is being computed using word embeddings:\n",
    "# \n",
    "\n",
    "m.most_similar(positive=['king','woman'], negative=['man'], topn=10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing most_similar on the king - man + woman example...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Paris', 0.7672387361526489),\n",
       " ('French', 0.6049168109893799),\n",
       " ('Parisian', 0.5810437202453613),\n",
       " ('Brussels', 0.542099118232727),\n",
       " ('Rome', 0.5099510550498962),\n",
       " ('Strasbourg', 0.5049293637275696),\n",
       " ('Marseilles', 0.49816644191741943),\n",
       " ('Toulouse', 0.4843180179595947),\n",
       " ('Paix', 0.4830804169178009),\n",
       " ('Francois', 0.4801149368286133)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \n",
    "# This problem is about building and testing analogies...\n",
    "# \n",
    "# This function has a hard-coded set of words, i.e., 'woman', 'king', and 'man'\n",
    "# Your tasks:\n",
    "#      + add inputs to the function \n",
    "#\n",
    "def test_most_similar(m):\n",
    "    \"\"\" example of most_similar \"\"\"\n",
    "    print(\"Testing most_similar on the king - man + woman example...\")\n",
    "    results = m.most_similar(positive=['woman', 'king'], negative=['man'], topn=10) # topn == # of results\n",
    "    results = m.most_similar(positive=['France', 'Berlin'], negative=['Germany'], topn=10) # topn == # of results\n",
    "    return results\n",
    "\n",
    "hard_coded_results = test_most_similar(m)\n",
    "hard_coded_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to do a few things here:\n",
    "# (A) Write generate_analogy(word1, word2, word3, m) and try it on some examples of your own.\n",
    "#     if word4 is the return value, then the idea is  word1 : word2 :: word3 : word4\n",
    "#          Warning:  the ordering of the words in the most_similar call is DIFFERENT (this is key)\n",
    "#\n",
    "#     Also, include a check that all of the words are in the model, e.g., adapting this:\n",
    "\"\"\"\n",
    "        if word not in model:  # or, not in m\n",
    "            print(f\"Aargh - the model does not contain {word}\")\n",
    "            return 'python' # or a suitable alternative\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seoul\n",
      "jazz\n",
      "blue\n",
      "chilly\n"
     ]
    }
   ],
   "source": [
    "def generate_analogy(w1, w2, w3, m):\n",
    "  \"\"\" returns word2vec's \"answer\" for w1:w2 :: w3:?? \"\"\"\n",
    "  if w1 not in m:  # example check\n",
    "    print(f\"{w1} was not in the model.\")\n",
    "    return 'Instead, we will return CS35'\n",
    "  elif w2 not in m:\n",
    "    print(f\"{w2} was not in the model.\")\n",
    "    return 'Instead, we will return CS35'\n",
    "  elif w3 not in m:\n",
    "    print(f\"{w3} was not in the model.\")\n",
    "    return 'Instead, we will return CS35'\n",
    "  else:\n",
    "    results = m.most_similar(positive=[f'{w3}', f'{w2}'], negative=[f'{w1}'], topn=1)\n",
    "    return results[0][0]\n",
    "    \n",
    "a = generate_analogy('California', 'Sacramento', 'Korea', m)    # WORKS!\n",
    "b = generate_analogy('sports', 'basketball', 'music', m)        # WORKS!\n",
    "c = generate_analogy('apple','red', 'watermelon', m)            # DOESN'T WORK!\n",
    "d = generate_analogy('jacket', 'cold', 'sandals', m)            # DOESN'T WORK!\n",
    "print(a)\n",
    "print(b)\n",
    "print(c)\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# (B) Write check_analogy(word1, word2, word3, word4, model) to return a \"score\" on how well word2vec_model\n",
    "#     does at solving the analogy provided, i.e.,    word1 : word2 :: word3 : word4\n",
    "#     + it should determine where word4 appears in the top 100 (use topn=100) most-similar words\n",
    "#     + if it _doens't_ appear in the top-100, it should give a score of 0\n",
    "#     + if it _does_ appear, it should give a score between 1 and 100, but\n",
    "#          it should be the distance from the _far_ end of the list. \n",
    "#     + Thus, a score of 100 means a perfect score. \n",
    "#     + A score of 1 means that word4 was the 100th in the list (index 99)\n",
    "#     + Try it out:   check_analogy( \"man\", \"king\", \"woman\", \"queen\", m ) -> 100\n",
    "#                     check_analogy( \"woman\", \"man\", \"bicycle\", \"fish\", m ) -> 0\n",
    "#                     check_analogy( \"woman\", \"man\", \"bicycle\", \"pedestrian\", m ) -> 96"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "0\n",
      "96\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# our check_analogy function\n",
    "#\n",
    "\n",
    "def check_analogy(word1, word2, word3, word4, model):\n",
    "    \"\"\" check_analogy's docstring - be sure to include it!\n",
    "    \"\"\"\n",
    "    if word1 not in m:  # example check\n",
    "        print(f\"{word1} was not in the model.\")\n",
    "        return 'Instead, we will return CS35'\n",
    "    elif word2 not in m:\n",
    "        print(f\"{word2} was not in the model.\")\n",
    "        return 'Instead, we will return CS35'\n",
    "    elif word3 not in m:\n",
    "        print(f\"{word3} was not in the model.\")\n",
    "        return 'Instead, we will return CS35'\n",
    "    else:\n",
    "        results = m.most_similar(positive=[f'{word3}', f'{word2}'], negative=[f'{word1}'], topn=100)\n",
    "        for item in results:\n",
    "            if item[0] == word4:\n",
    "                return 100 - int(results.index(item))\n",
    "        return 0\n",
    "\n",
    "a1 = check_analogy( \"man\", \"king\", \"woman\", \"queen\", m )\n",
    "a2 = check_analogy( \"woman\", \"man\", \"bicycle\", \"fish\", m )\n",
    "a3 = check_analogy( \"woman\", \"man\", \"bicycle\", \"pedestrian\", m )\n",
    "\n",
    "print(a1)\n",
    "print(a2)\n",
    "print(a3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "100\n",
      "0\n",
      "0\n",
      "\n",
      "\n",
      "100\n",
      "90\n",
      "42\n",
      "25\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Results and commentary...\n",
    "#\n",
    "\n",
    "b1 = check_analogy('apple', 'red', 'blueberry', 'blue', m)\n",
    "print(b1)\n",
    "\n",
    "b2 = check_analogy('sun', 'summer', 'ice', 'winter', m)\n",
    "print(b2)\n",
    "\n",
    "b3 = check_analogy('potato', 'vegetable', 'tomato', 'spaceship', m)\n",
    "print(b3)\n",
    "\n",
    "b4 = check_analogy('caramel', 'sweet', 'medicine', 'sword', m)\n",
    "print(b4)\n",
    "\n",
    "\n",
    "print()\n",
    "print()\n",
    "\n",
    "# Trial and Error...\n",
    "\n",
    "c1 = check_analogy('ears', 'hear', 'eyes', 'see', m)\n",
    "print(c1)\n",
    "c2 = check_analogy('sun', 'yellow', 'snow', 'white', m)\n",
    "print(c2)\n",
    "c3 = check_analogy('crackers', 'snack', 'pork', 'breakfast', m)\n",
    "print(c3)\n",
    "c4 = check_analogy('Japan', 'Tokyo', 'America', 'Texas', m)\n",
    "print(c4)\n",
    "c5 = check_analogy('coffee', 'drink', 'soda', 'pizza', m)\n",
    "print(c5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "1f0c8735478a48ff7ef3deb8c421f15aa5d573c59a98bc92eb9b829f28c47b33"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
