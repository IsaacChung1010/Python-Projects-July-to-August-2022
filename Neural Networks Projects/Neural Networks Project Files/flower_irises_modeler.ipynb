{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "####   hw6pr1iris_modeler \n",
    "+ iris clasification and regression via NNets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# hw6pr1iris_modeler:  NNETS! \n",
    "#\n",
    "#   including _both_ clasification + regression for iris-species modeling\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries...\n",
    "import numpy as np      # numpy is Python's \"array\" library\n",
    "import pandas as pd     # Pandas is Python's \"data\" library (\"dataframe\" == spreadsheet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iris_cleaned.csv : file read into a pandas dataframe.\n"
     ]
    }
   ],
   "source": [
    "# let's read in our flower data...\n",
    "# \n",
    "# iris_cleaned.csv and hw4pr1iris_cleaner.ipynb should be in this folder\n",
    "# \n",
    "filename = 'iris_cleaned.csv'\n",
    "df_tidy = pd.read_csv(filename)      # encoding = \"utf-8\", \"latin1\"\n",
    "print(f\"{filename} : file read into a pandas dataframe.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_tidy.shape is (141, 6)\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 141 entries, 0 to 140\n",
      "Data columns (total 6 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   sepallen  141 non-null    float64\n",
      " 1   sepalwid  141 non-null    float64\n",
      " 2   petallen  141 non-null    float64\n",
      " 3   petalwid  141 non-null    float64\n",
      " 4   irisname  141 non-null    object \n",
      " 5   irisnum   141 non-null    int64  \n",
      "dtypes: float64(4), int64(1), object(1)\n",
      "memory usage: 7.7+ KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepallen</th>\n",
       "      <th>sepalwid</th>\n",
       "      <th>petallen</th>\n",
       "      <th>petalwid</th>\n",
       "      <th>irisname</th>\n",
       "      <th>irisnum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>setosa</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.8</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>7.9</td>\n",
       "      <td>3.8</td>\n",
       "      <td>6.4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>virginica</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>7.6</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.6</td>\n",
       "      <td>2.1</td>\n",
       "      <td>virginica</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>7.7</td>\n",
       "      <td>3.8</td>\n",
       "      <td>6.7</td>\n",
       "      <td>2.2</td>\n",
       "      <td>virginica</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>7.7</td>\n",
       "      <td>2.8</td>\n",
       "      <td>6.7</td>\n",
       "      <td>2.0</td>\n",
       "      <td>virginica</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>7.7</td>\n",
       "      <td>2.6</td>\n",
       "      <td>6.9</td>\n",
       "      <td>2.3</td>\n",
       "      <td>virginica</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>141 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepallen  sepalwid  petallen  petalwid   irisname  irisnum\n",
       "0         4.6       3.6       1.0       0.2     setosa        0\n",
       "1         4.3       3.0       1.1       0.1     setosa        0\n",
       "2         5.0       3.2       1.2       0.2     setosa        0\n",
       "3         5.8       4.0       1.2       0.2     setosa        0\n",
       "4         4.4       3.0       1.3       0.2     setosa        0\n",
       "..        ...       ...       ...       ...        ...      ...\n",
       "136       7.9       3.8       6.4       2.0  virginica        2\n",
       "137       7.6       3.0       6.6       2.1  virginica        2\n",
       "138       7.7       3.8       6.7       2.2  virginica        2\n",
       "139       7.7       2.8       6.7       2.0  virginica        2\n",
       "140       7.7       2.6       6.9       2.3  virginica        2\n",
       "\n",
       "[141 rows x 6 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "# different version vary on how to see all rows (adapt to suit your system!)\n",
    "#\n",
    "print(f\"df_tidy.shape is {df_tidy.shape}\\n\")\n",
    "df_tidy.info()  # prints column information\n",
    "\n",
    "# let's print the whole dataframe, too  (adapt # of lines, as desired)\n",
    "# pd.options.display.max_rows = 150   # None for no limit; default: 10\n",
    "# pd.options.display.min_rows = 150   # None for no limit; default: 10\n",
    "# pd.options.display.max_rows = 10   # None for no limit; default: 10\n",
    "# pd.options.display.min_rows = 10   # None for no limit; default: 10\n",
    "df_tidy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepallen</th>\n",
       "      <th>sepalwid</th>\n",
       "      <th>petallen</th>\n",
       "      <th>petalwid</th>\n",
       "      <th>irisnum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.8</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>7.9</td>\n",
       "      <td>3.8</td>\n",
       "      <td>6.4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>7.6</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.6</td>\n",
       "      <td>2.1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>7.7</td>\n",
       "      <td>3.8</td>\n",
       "      <td>6.7</td>\n",
       "      <td>2.2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>7.7</td>\n",
       "      <td>2.8</td>\n",
       "      <td>6.7</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>7.7</td>\n",
       "      <td>2.6</td>\n",
       "      <td>6.9</td>\n",
       "      <td>2.3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>141 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepallen  sepalwid  petallen  petalwid  irisnum\n",
       "0         4.6       3.6       1.0       0.2        0\n",
       "1         4.3       3.0       1.1       0.1        0\n",
       "2         5.0       3.2       1.2       0.2        0\n",
       "3         5.8       4.0       1.2       0.2        0\n",
       "4         4.4       3.0       1.3       0.2        0\n",
       "..        ...       ...       ...       ...      ...\n",
       "136       7.9       3.8       6.4       2.0        2\n",
       "137       7.6       3.0       6.6       2.1        2\n",
       "138       7.7       3.8       6.7       2.2        2\n",
       "139       7.7       2.8       6.7       2.0        2\n",
       "140       7.7       2.6       6.9       2.3        2\n",
       "\n",
       "[141 rows x 5 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "# All of the columns need to be numeric, we'll drop irisname\n",
    "ROW = 0\n",
    "COLUMN = 1\n",
    "df_model1 = df_tidy.drop( 'irisname', axis=COLUMN )\n",
    "df_model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COLUMNS is Index(['sepallen', 'sepalwid', 'petallen', 'petalwid', 'irisnum'], dtype='object')\n",
      "\n",
      "COLUMNS[0] is sepallen\n",
      "\n",
      "COL_INDEX is {'sepallen': 0, 'sepalwid': 1, 'petallen': 2, 'petalwid': 3, 'irisnum': 4}\n",
      "\n",
      "\n",
      "setosa maps to 0\n",
      "versicolor maps to 1\n",
      "virginica maps to 2\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# once we have all the columns we want, let's create an index of their names...\n",
    "\n",
    "#\n",
    "# Let's make sure we have all of our helpful variables in one place \n",
    "#       To be adapted if we drop/add more columns...\n",
    "#\n",
    "\n",
    "#\n",
    "# let's keep our column names in variables, for reference\n",
    "#\n",
    "COLUMNS = df_model1.columns            # \"list\" of columns\n",
    "print(f\"COLUMNS is {COLUMNS}\\n\")  \n",
    "  # It's a \"pandas\" list, called an Index\n",
    "  # use it just as a Python list of strings:\n",
    "print(f\"COLUMNS[0] is {COLUMNS[0]}\\n\")\n",
    "\n",
    "# let's create a dictionary to look up any column index by name\n",
    "COL_INDEX = {}\n",
    "for i, name in enumerate(COLUMNS):\n",
    "    COL_INDEX[name] = i  # using the name (as key), look up the value (i)\n",
    "print(f\"COL_INDEX is {COL_INDEX}\\n\\n\")\n",
    "\n",
    "\n",
    "#\n",
    "# and our \"species\" names\n",
    "#\n",
    "\n",
    "# all of scikit-learn's ML routines need numbers, not strings\n",
    "#   ... even for categories/classifications (like species!)\n",
    "#   so, we will convert the flower-species to numbers:\n",
    "\n",
    "SPECIES = ['setosa','versicolor','virginica']   # int to str\n",
    "SPECIES_INDEX = {'setosa':0,'versicolor':1,'virginica':2}  # str to int\n",
    "\n",
    "# Let's try it out...\n",
    "for name in SPECIES:\n",
    "    print(f\"{name} maps to {SPECIES_INDEX[name]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4.6 3.6 1.  0.2 0. ]\n",
      " [4.3 3.  1.1 0.1 0. ]\n",
      " [5.  3.2 1.2 0.2 0. ]\n",
      " [5.8 4.  1.2 0.2 0. ]\n",
      " [4.4 3.  1.3 0.2 0. ]\n",
      " [4.4 3.2 1.3 0.2 0. ]\n",
      " [4.5 2.3 1.3 0.3 0. ]\n",
      " [4.7 3.2 1.3 0.2 0. ]\n",
      " [5.  3.5 1.3 0.3 0. ]\n",
      " [5.4 3.9 1.3 0.4 0. ]\n",
      " [5.5 3.5 1.3 0.2 0. ]\n",
      " [4.4 2.9 1.4 0.2 0. ]\n",
      " [4.6 3.4 1.4 0.3 0. ]\n",
      " [4.6 3.2 1.4 0.2 0. ]\n",
      " [4.8 3.  1.4 0.1 0. ]\n",
      " [4.8 3.  1.4 0.3 0. ]\n",
      " [4.9 3.  1.4 0.2 0. ]\n",
      " [5.  3.6 1.4 0.2 0. ]\n",
      " [5.  3.3 1.4 0.2 0. ]\n",
      " [5.1 3.5 1.4 0.2 0. ]\n",
      " [5.1 3.5 1.4 0.3 0. ]\n",
      " [5.2 3.4 1.4 0.2 0. ]\n",
      " [5.5 4.2 1.4 0.2 0. ]\n",
      " [4.6 3.1 1.5 0.2 0. ]\n",
      " [4.9 3.1 1.5 0.1 0. ]\n",
      " [4.9 3.1 1.5 0.1 0. ]\n",
      " [4.9 3.1 1.5 0.1 0. ]\n",
      " [5.  3.4 1.5 0.2 0. ]\n",
      " [5.1 3.8 1.5 0.3 0. ]\n",
      " [5.1 3.7 1.5 0.4 0. ]\n",
      " [5.1 3.4 1.5 0.2 0. ]\n",
      " [5.2 3.5 1.5 0.2 0. ]\n",
      " [5.3 3.7 1.5 0.2 0. ]\n",
      " [5.4 3.7 1.5 0.2 0. ]\n",
      " [5.7 4.4 1.5 0.4 0. ]\n",
      " [4.7 3.2 1.6 0.2 0. ]\n",
      " [4.8 3.4 1.6 0.2 0. ]\n",
      " [5.  3.  1.6 0.2 0. ]\n",
      " [5.  3.4 1.6 0.4 0. ]\n",
      " [5.  3.5 1.6 0.6 0. ]\n",
      " [5.1 3.8 1.6 0.2 0. ]\n",
      " [5.1 3.3 1.7 0.5 0. ]\n",
      " [5.4 3.9 1.7 0.4 0. ]\n",
      " [5.4 3.4 1.7 0.2 0. ]\n",
      " [5.7 3.8 1.7 0.3 0. ]\n",
      " [4.8 3.4 1.9 0.2 0. ]\n",
      " [5.1 3.8 1.9 0.4 0. ]\n",
      " [7.  3.2 4.7 1.4 1. ]\n",
      " [4.9 2.4 3.3 1.  1. ]\n",
      " [5.  2.3 3.3 1.  1. ]\n",
      " [5.  2.  3.5 1.  1. ]\n",
      " [5.7 2.6 3.5 1.  1. ]\n",
      " [5.6 2.9 3.6 1.3 1. ]\n",
      " [5.5 2.4 3.7 1.  1. ]\n",
      " [5.5 2.4 3.8 1.1 1. ]\n",
      " [5.2 2.7 3.9 1.4 1. ]\n",
      " [5.6 2.5 3.9 1.1 1. ]\n",
      " [5.8 2.7 3.9 1.2 1. ]\n",
      " [5.5 2.3 4.  1.3 1. ]\n",
      " [5.5 2.5 4.  1.3 1. ]\n",
      " [5.8 2.6 4.  1.2 1. ]\n",
      " [6.  2.2 4.  1.  1. ]\n",
      " [6.1 2.8 4.  1.3 1. ]\n",
      " [5.6 3.  4.1 1.3 1. ]\n",
      " [5.8 2.7 4.1 1.  1. ]\n",
      " [5.6 2.7 4.2 1.3 1. ]\n",
      " [5.7 3.  4.2 1.2 1. ]\n",
      " [5.9 3.  4.2 1.5 1. ]\n",
      " [6.4 2.9 4.3 1.3 1. ]\n",
      " [5.5 2.6 4.4 1.2 1. ]\n",
      " [6.3 2.3 4.4 1.3 1. ]\n",
      " [6.6 3.  4.4 1.4 1. ]\n",
      " [6.7 3.1 4.4 1.4 1. ]\n",
      " [5.4 3.  4.5 1.5 1. ]\n",
      " [5.6 3.  4.5 1.5 1. ]\n",
      " [5.7 2.8 4.5 1.3 1. ]\n",
      " [6.  2.9 4.5 1.5 1. ]\n",
      " [6.  3.4 4.5 1.6 1. ]\n",
      " [6.2 2.2 4.5 1.5 1. ]\n",
      " [6.4 3.2 4.5 1.5 1. ]\n",
      " [6.1 3.  4.6 1.4 1. ]\n",
      " [6.5 2.8 4.6 1.5 1. ]\n",
      " [6.6 2.9 4.6 1.3 1. ]\n",
      " [6.1 2.9 4.7 1.4 1. ]\n",
      " [6.1 2.8 4.7 1.2 1. ]\n",
      " [6.3 3.3 4.7 1.6 1. ]\n",
      " [6.7 3.1 4.7 1.5 1. ]\n",
      " [5.9 3.2 4.8 1.8 1. ]\n",
      " [6.8 2.8 4.8 1.4 1. ]\n",
      " [6.3 2.5 4.9 1.5 1. ]\n",
      " [6.9 3.1 4.9 1.5 1. ]\n",
      " [6.7 3.  5.  1.7 1. ]\n",
      " [6.  2.7 5.1 1.6 1. ]\n",
      " [4.9 2.5 4.5 1.7 2. ]\n",
      " [6.  3.  4.8 1.8 2. ]\n",
      " [6.2 2.8 4.8 1.8 2. ]\n",
      " [5.6 2.8 4.9 2.  2. ]\n",
      " [6.1 3.  4.9 1.8 2. ]\n",
      " [6.3 2.7 4.9 1.8 2. ]\n",
      " [5.7 2.5 5.  2.  2. ]\n",
      " [6.  2.2 5.  1.5 2. ]\n",
      " [6.3 2.5 5.  1.9 2. ]\n",
      " [5.8 2.8 5.1 2.4 2. ]\n",
      " [5.8 2.7 5.1 1.9 2. ]\n",
      " [5.9 3.  5.1 1.8 2. ]\n",
      " [6.3 2.8 5.1 1.5 2. ]\n",
      " [6.5 3.2 5.1 2.  2. ]\n",
      " [6.9 3.1 5.1 2.3 2. ]\n",
      " [6.5 3.  5.2 2.  2. ]\n",
      " [6.7 3.  5.2 2.3 2. ]\n",
      " [6.4 2.7 5.3 1.9 2. ]\n",
      " [6.4 3.2 5.3 2.3 2. ]\n",
      " [6.2 3.4 5.4 2.3 2. ]\n",
      " [6.9 3.1 5.4 2.1 2. ]\n",
      " [6.4 3.1 5.5 1.8 2. ]\n",
      " [6.5 3.  5.5 1.8 2. ]\n",
      " [6.8 3.  5.5 2.1 2. ]\n",
      " [6.1 2.6 5.6 1.4 2. ]\n",
      " [6.3 2.9 5.6 1.8 2. ]\n",
      " [6.3 3.4 5.6 2.4 2. ]\n",
      " [6.4 2.8 5.6 2.1 2. ]\n",
      " [6.4 2.8 5.6 2.2 2. ]\n",
      " [6.7 3.1 5.6 2.4 2. ]\n",
      " [6.7 3.3 5.7 2.1 2. ]\n",
      " [6.7 3.3 5.7 2.5 2. ]\n",
      " [6.9 3.2 5.7 2.3 2. ]\n",
      " [6.5 3.  5.8 2.2 2. ]\n",
      " [6.7 2.5 5.8 1.8 2. ]\n",
      " [7.2 3.  5.8 1.6 2. ]\n",
      " [6.8 3.2 5.9 2.3 2. ]\n",
      " [7.1 3.  5.9 2.1 2. ]\n",
      " [7.2 3.2 6.  1.8 2. ]\n",
      " [7.2 3.6 6.1 2.5 2. ]\n",
      " [7.4 2.8 6.1 1.9 2. ]\n",
      " [7.7 3.  6.1 2.3 2. ]\n",
      " [7.3 2.9 6.3 1.8 2. ]\n",
      " [7.9 3.8 6.4 2.  2. ]\n",
      " [7.6 3.  6.6 2.1 2. ]\n",
      " [7.7 3.8 6.7 2.2 2. ]\n",
      " [7.7 2.8 6.7 2.  2. ]\n",
      " [7.7 2.6 6.9 2.3 2. ]]\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# let's convert our dataframe to a numpy array, named A\n",
    "#\n",
    "A = df_model1.to_numpy()   \n",
    "A = A.astype('float64')    # many types:  www.tutorialspoint.com/numpy/numpy_data_types.htm\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The dataset has 141 rows and 5 cols\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# nice to have NUM_ROWS and NUM_COLS around\n",
    "#\n",
    "NUM_ROWS, NUM_COLS = A.shape\n",
    "print(f\"\\nThe dataset has {NUM_ROWS} rows and {NUM_COLS} cols\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flower #42 is [5.4 3.9 1.7 0.4 0. ]\n",
      "  Its sepallen is 5.4\n",
      "  Its sepalwid is 3.9\n",
      "  Its petallen is 1.7\n",
      "  Its petalwid is 0.4\n",
      "  Its irisnum is 0.0\n",
      "  Its species is setosa (i.e., 0)\n"
     ]
    }
   ],
   "source": [
    "# let's use all of our variables, to reinforce that we have\n",
    "# (1) names...\n",
    "# (2) access and control...\n",
    "\n",
    "# choose a row index, n:\n",
    "n = 42\n",
    "print(f\"flower #{n} is {A[n]}\")\n",
    "\n",
    "for i in range(len(COLUMNS)):\n",
    "    colname = COLUMNS[i]\n",
    "    value = A[n][i]\n",
    "    print(f\"  Its {colname} is {value}\")\n",
    "\n",
    "species_index = COL_INDEX['irisnum']\n",
    "species_num = int(round(A[n][species_index]))\n",
    "species = SPECIES[species_num]\n",
    "print(f\"  Its species is {species} (i.e., {species_num})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+++ Start of data definitions +++\n",
      "\n",
      "y_all (just the labels/species)   are \n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.]\n",
      "X_all (just the features, first few rows) are \n",
      " [[4.6 3.6 1.  0.2]\n",
      " [4.3 3.  1.1 0.1]\n",
      " [5.  3.2 1.2 0.2]\n",
      " [5.8 4.  1.2 0.2]\n",
      " [4.4 3.  1.3 0.2]]\n"
     ]
    }
   ],
   "source": [
    "print(\"+++ Start of data definitions +++\\n\")\n",
    "\n",
    "#\n",
    "# we could do this at the data-frame level, too!\n",
    "#\n",
    "\n",
    "X_all = A[:,0:4]  # X (features) ... is all rows, columns 0, 1, 2, 3\n",
    "y_all = A[:,4]    # y (labels) ... is all rows, column 4 only\n",
    "\n",
    "print(f\"y_all (just the labels/species)   are \\n {y_all}\")\n",
    "print(f\"X_all (just the features, first few rows) are \\n {X_all[0:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The scrambled labels/species are \n",
      " [2. 2. 2. 1. 0. 2. 2. 0. 0. 1. 1. 2. 0. 2. 2. 0. 2. 2. 2. 2. 1. 1. 0. 0.\n",
      " 1. 2. 2. 0. 1. 0. 0. 1. 0. 1. 2. 1. 2. 1. 2. 0. 0. 2. 1. 2. 1. 1. 2. 2.\n",
      " 2. 1. 2. 2. 1. 2. 2. 1. 1. 1. 2. 0. 0. 1. 0. 2. 2. 1. 2. 1. 0. 0. 2. 0.\n",
      " 0. 1. 0. 2. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 2. 2. 1. 1. 1. 0. 0. 0. 1.\n",
      " 2. 1. 1. 2. 0. 2. 0. 0. 1. 0. 2. 0. 2. 0. 1. 0. 1. 1. 0. 0. 0. 1. 2. 2.\n",
      " 2. 1. 2. 2. 0. 1. 1. 0. 2. 0. 1. 1. 0. 0. 2. 1. 1. 0. 1. 2. 2.]\n",
      "The corresponding data rows are \n",
      " [[6.4 2.7 5.3 1.9]\n",
      " [7.7 2.8 6.7 2. ]\n",
      " [6.4 2.8 5.6 2.2]\n",
      " [5.5 2.5 4.  1.3]\n",
      " [4.8 3.4 1.9 0.2]]\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# we can scramble the data, to remove (potential) dependence on its ordering: \n",
    "# \n",
    "indices = np.random.permutation(len(y_all))  # indices is a permutation-list\n",
    "\n",
    "# we scramble both X and y, necessarily with the same permutation\n",
    "X_all = X_all[indices]              # we apply the _same_ permutation to each!\n",
    "y_all = y_all[indices]              # again...\n",
    "print(f\"The scrambled labels/species are \\n {y_all}\")\n",
    "print(f\"The corresponding data rows are \\n {X_all[0:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training with 112 rows;  testing with 29 rows\n",
      "\n",
      "Held-out data... (testing data: 29)\n",
      "y_test: [0. 1. 0. 2. 1. 2. 1. 0. 0. 1. 1. 1. 1. 2. 1. 0. 2. 1. 0. 0. 0. 0. 1. 2.\n",
      " 2. 1. 2. 1. 0.]\n",
      "\n",
      "X_test (few rows): [[5.  3.4 1.6 0.4]\n",
      " [5.6 3.  4.5 1.5]\n",
      " [5.1 3.8 1.9 0.4]\n",
      " [6.1 3.  4.9 1.8]\n",
      " [5.5 2.6 4.4 1.2]]\n",
      "\n",
      "Data used for modeling... (training data: 112)\n",
      "y_train: [0. 0. 0. 2. 0. 0. 2. 0. 1. 1. 1. 1. 0. 2. 2. 2. 0. 2. 2. 0. 1. 1. 2. 2.\n",
      " 2. 0. 0. 0. 0. 0. 0. 2. 0. 1. 2. 0. 2. 1. 0. 2. 2. 2. 2. 1. 0. 1. 1. 2.\n",
      " 1. 0. 0. 1. 2. 1. 1. 2. 0. 0. 0. 1. 2. 2. 2. 2. 2. 2. 1. 0. 0. 1. 1. 1.\n",
      " 1. 2. 0. 2. 0. 1. 1. 0. 2. 2. 2. 2. 1. 2. 0. 1. 0. 1. 0. 0. 1. 2. 0. 1.\n",
      " 2. 2. 1. 2. 2. 1. 0. 1. 0. 2. 1. 1. 1. 2. 0. 2.]\n",
      "\n",
      "X_train (few rows): [[5.1 3.5 1.4 0.2]\n",
      " [4.4 3.2 1.3 0.2]\n",
      " [4.8 3.  1.4 0.1]\n",
      " [6.4 2.8 5.6 2.2]\n",
      " [5.  3.4 1.5 0.2]]\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# We next separate into test data and training data ... \n",
    "#    + We will train on the training data...\n",
    "#    + We will _not_ look at the testing data to build the model\n",
    "#\n",
    "# Then, afterward, we will test on the testing data -- and see how well we do!\n",
    "#\n",
    "\n",
    "#\n",
    "# a common convention:  train on 80%, test on 20%    Let's define the TEST_PERCENT\n",
    "#\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_all, y_all, test_size=0.2)\n",
    "\n",
    "print(f\"training with {len(y_train)} rows;  testing with {len(y_test)} rows\\n\" )\n",
    "\n",
    "print(f\"Held-out data... (testing data: {len(y_test)})\")\n",
    "print(f\"y_test: {y_test}\\n\")\n",
    "print(f\"X_test (few rows): {X_test[0:5,:]}\")  # 5 rows\n",
    "print()\n",
    "print(f\"Data used for modeling... (training data: {len(y_train)})\")\n",
    "print(f\"y_train: {y_train}\\n\")\n",
    "print(f\"X_train (few rows): {X_train[0:5,:]}\")  # 5 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# for NNets, it's important to keep the feature values near 0, say -1. to 1. or so\n",
    "#    This is done through the \"StandardScaler\" in scikit-learn\n",
    "# \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "USE_SCALER = True   # this variable is important! It tracks if we need to use the scaler...\n",
    "\n",
    "# we \"train the scaler\"  (computes the mean and standard deviation)\n",
    "if USE_SCALER == True:\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_train)  # Scale with the training data! ave becomes 0; stdev becomes 1\n",
    "else:\n",
    "    # this one does no scaling!  We still create it to be consistent:\n",
    "    scaler = StandardScaler(copy=True, with_mean=False, with_std=False) # no scaling\n",
    "    scaler.fit(X_train)  # still need to fit, though it does not change...\n",
    "\n",
    "scaler   # is now defined and ready to use...\n",
    "\n",
    "# ++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "\n",
    "# Here are our scaled training and testing sets:\n",
    "\n",
    "X_train_scaled = scaler.transform(X_train) # scale!\n",
    "X_test_scaled = scaler.transform(X_test) # scale!\n",
    "\n",
    "y_train_scaled = y_train  # the predicted/desired labels are not scaled\n",
    "y_test_scaled = y_test  # not using the scaler\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    input  -> pred  des. \n",
      "         [-0.89971416  1.02968317 -1.32964419 -1.32421551] -> ?     0.00 \n",
      "         [-1.71423599  0.3499709  -1.38500484 -1.32421551] -> ?     0.00 \n",
      "         [-1.24879494 -0.10317061 -1.32964419 -1.45465714] -> ?     0.00 \n",
      "         [ 0.61296923 -0.55631212  0.99550312  1.28461716] -> ?     2.00 \n",
      "         [-1.01607442  0.80311241 -1.27428354 -1.32421551] -> ?     0.00 \n",
      "\n",
      "                                                    input  -> pred  des. \n",
      "                                         [5.1 3.5 1.4 0.2] -> ?     0.00 \n",
      "                                         [4.4 3.2 1.3 0.2] -> ?     0.00 \n",
      "                                         [4.8 3.  1.4 0.1] -> ?     0.00 \n",
      "                                         [6.4 2.8 5.6 2.2] -> ?     2.00 \n",
      "                                         [5.  3.4 1.5 0.2] -> ?     0.00 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# let's create a table for showing our data and its predictions...\n",
    "#\n",
    "def ascii_table(X,y,scaler_to_invert=None):\n",
    "    \"\"\" print a table of binary inputs and outputs \"\"\"\n",
    "    if scaler_to_invert == None:  # don't use the scaler\n",
    "        X = X\n",
    "    else:\n",
    "        X = scaler_to_invert.inverse_transform(X)\n",
    "    print(f\"{'input ':>58s} -> {'pred':<5s} {'des.':<5s}\") \n",
    "    for i in range(len(y)):\n",
    "        print(f\"{X[i,0:4]!s:>58s} -> {'?':<5s} {y[i]:<5.2f}\")   # !s is str ...\n",
    "    print()\n",
    "    \n",
    "# to show the table with the scaled data:\n",
    "ascii_table(X_train_scaled[0:5,:],y_train_scaled[0:5],None)\n",
    "\n",
    "# to show the table with the original data:\n",
    "ascii_table(X_train_scaled[0:5,:],y_train_scaled[0:5],scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "++++++++++  TRAINING:  begin  +++++++++++++++\n",
      "\n",
      "\n",
      "Iteration 1, loss = 1.22028668\n",
      "Iteration 2, loss = 1.05884133\n",
      "Iteration 3, loss = 0.90277548\n",
      "Iteration 4, loss = 0.77298412\n",
      "Iteration 5, loss = 0.67099173\n",
      "Iteration 6, loss = 0.59664928\n",
      "Iteration 7, loss = 0.54580306\n",
      "Iteration 8, loss = 0.50979849\n",
      "Iteration 9, loss = 0.48161161\n",
      "Iteration 10, loss = 0.45818486\n",
      "Iteration 11, loss = 0.43854263\n",
      "Iteration 12, loss = 0.42176247\n",
      "Iteration 13, loss = 0.40654668\n",
      "Iteration 14, loss = 0.39175781\n",
      "Iteration 15, loss = 0.37683378\n",
      "Iteration 16, loss = 0.36172731\n",
      "Iteration 17, loss = 0.34661114\n",
      "Iteration 18, loss = 0.33164952\n",
      "Iteration 19, loss = 0.31693372\n",
      "Iteration 20, loss = 0.30251397\n",
      "Iteration 21, loss = 0.28843280\n",
      "Iteration 22, loss = 0.27471959\n",
      "Iteration 23, loss = 0.26135882\n",
      "Iteration 24, loss = 0.24826099\n",
      "Iteration 25, loss = 0.23525326\n",
      "Iteration 26, loss = 0.22209160\n",
      "Iteration 27, loss = 0.20849216\n",
      "Iteration 28, loss = 0.19418936\n",
      "Iteration 29, loss = 0.17903695\n",
      "Iteration 30, loss = 0.16315330\n",
      "Iteration 31, loss = 0.14705702\n",
      "Iteration 32, loss = 0.13166638\n",
      "Iteration 33, loss = 0.11804536\n",
      "Iteration 34, loss = 0.10696779\n",
      "Iteration 35, loss = 0.09860071\n",
      "Iteration 36, loss = 0.09254397\n",
      "Iteration 37, loss = 0.08812203\n",
      "Iteration 38, loss = 0.08465948\n",
      "Iteration 39, loss = 0.08161542\n",
      "Iteration 40, loss = 0.07861748\n",
      "Iteration 41, loss = 0.07546834\n",
      "Iteration 42, loss = 0.07214242\n",
      "Iteration 43, loss = 0.06874797\n",
      "Iteration 44, loss = 0.06545258\n",
      "Iteration 45, loss = 0.06240798\n",
      "Iteration 46, loss = 0.05970595\n",
      "Iteration 47, loss = 0.05736967\n",
      "Iteration 48, loss = 0.05536909\n",
      "Iteration 49, loss = 0.05364495\n",
      "Iteration 50, loss = 0.05212972\n",
      "Iteration 51, loss = 0.05076106\n",
      "Iteration 52, loss = 0.04948847\n",
      "Iteration 53, loss = 0.04827578\n",
      "Iteration 54, loss = 0.04710117\n",
      "Iteration 55, loss = 0.04595561\n",
      "Iteration 56, loss = 0.04484027\n",
      "Iteration 57, loss = 0.04376306\n",
      "Iteration 58, loss = 0.04273508\n",
      "Iteration 59, loss = 0.04176734\n",
      "Iteration 60, loss = 0.04086815\n",
      "Iteration 61, loss = 0.04004146\n",
      "Iteration 62, loss = 0.03928628\n",
      "Iteration 63, loss = 0.03859705\n",
      "Iteration 64, loss = 0.03796490\n",
      "Iteration 65, loss = 0.03737929\n",
      "Iteration 66, loss = 0.03682979\n",
      "Iteration 67, loss = 0.03630752\n",
      "Iteration 68, loss = 0.03580598\n",
      "Iteration 69, loss = 0.03532128\n",
      "Iteration 70, loss = 0.03485181\n",
      "Iteration 71, loss = 0.03439753\n",
      "Iteration 72, loss = 0.03395919\n",
      "Iteration 73, loss = 0.03353764\n",
      "Iteration 74, loss = 0.03313334\n",
      "Iteration 75, loss = 0.03274609\n",
      "Iteration 76, loss = 0.03237505\n",
      "Iteration 77, loss = 0.03201891\n",
      "Iteration 78, loss = 0.03167609\n",
      "Iteration 79, loss = 0.03134498\n",
      "Iteration 80, loss = 0.03102419\n",
      "Iteration 81, loss = 0.03071260\n",
      "Iteration 82, loss = 0.03040944\n",
      "Iteration 83, loss = 0.03011424\n",
      "Iteration 84, loss = 0.02982675\n",
      "Iteration 85, loss = 0.02954687\n",
      "Iteration 86, loss = 0.02927451\n",
      "Iteration 87, loss = 0.02900959\n",
      "Iteration 88, loss = 0.02875191\n",
      "Iteration 89, loss = 0.02850121\n",
      "Iteration 90, loss = 0.02825712\n",
      "Iteration 91, loss = 0.02801927\n",
      "Iteration 92, loss = 0.02778724\n",
      "Iteration 93, loss = 0.02756062\n",
      "Iteration 94, loss = 0.02733906\n",
      "Iteration 95, loss = 0.02712225\n",
      "Iteration 96, loss = 0.02690994\n",
      "Iteration 97, loss = 0.02670190\n",
      "Iteration 98, loss = 0.02649796\n",
      "Iteration 99, loss = 0.02629796\n",
      "Iteration 100, loss = 0.02610174\n",
      "Iteration 101, loss = 0.02590915\n",
      "Iteration 102, loss = 0.02572004\n",
      "Iteration 103, loss = 0.02553427\n",
      "Iteration 104, loss = 0.02535168\n",
      "Iteration 105, loss = 0.02517213\n",
      "Iteration 106, loss = 0.02499547\n",
      "Iteration 107, loss = 0.02482158\n",
      "Iteration 108, loss = 0.02465034\n",
      "Iteration 109, loss = 0.02448163\n",
      "Iteration 110, loss = 0.02431536\n",
      "Iteration 111, loss = 0.02415141\n",
      "Iteration 112, loss = 0.02398970\n",
      "Iteration 113, loss = 0.02383014\n",
      "Iteration 114, loss = 0.02367264\n",
      "Iteration 115, loss = 0.02351712\n",
      "Iteration 116, loss = 0.02336348\n",
      "Iteration 117, loss = 0.02321166\n",
      "Iteration 118, loss = 0.02306157\n",
      "Iteration 119, loss = 0.02291314\n",
      "Iteration 120, loss = 0.02276629\n",
      "Iteration 121, loss = 0.02262098\n",
      "Iteration 122, loss = 0.02247712\n",
      "Iteration 123, loss = 0.02233467\n",
      "Iteration 124, loss = 0.02219356\n",
      "Iteration 125, loss = 0.02205375\n",
      "Iteration 126, loss = 0.02191519\n",
      "Iteration 127, loss = 0.02177782\n",
      "Iteration 128, loss = 0.02164162\n",
      "Iteration 129, loss = 0.02150652\n",
      "Iteration 130, loss = 0.02137249\n",
      "Iteration 131, loss = 0.02123950\n",
      "Iteration 132, loss = 0.02110751\n",
      "Iteration 133, loss = 0.02097648\n",
      "Iteration 134, loss = 0.02084639\n",
      "Iteration 135, loss = 0.02071719\n",
      "Iteration 136, loss = 0.02058888\n",
      "Iteration 137, loss = 0.02046141\n",
      "Iteration 138, loss = 0.02033477\n",
      "Iteration 139, loss = 0.02020894\n",
      "Iteration 140, loss = 0.02008388\n",
      "Iteration 141, loss = 0.01995959\n",
      "Iteration 142, loss = 0.01983604\n",
      "Iteration 143, loss = 0.01971321\n",
      "Iteration 144, loss = 0.01959110\n",
      "Iteration 145, loss = 0.01946968\n",
      "Iteration 146, loss = 0.01934895\n",
      "Iteration 147, loss = 0.01922889\n",
      "Iteration 148, loss = 0.01910949\n",
      "Iteration 149, loss = 0.01899074\n",
      "Iteration 150, loss = 0.01887263\n",
      "Iteration 151, loss = 0.01875515\n",
      "Iteration 152, loss = 0.01863830\n",
      "Iteration 153, loss = 0.01852207\n",
      "Iteration 154, loss = 0.01840646\n",
      "Iteration 155, loss = 0.01829145\n",
      "Iteration 156, loss = 0.01817704\n",
      "Iteration 157, loss = 0.01806324\n",
      "Iteration 158, loss = 0.01795003\n",
      "Iteration 159, loss = 0.01783741\n",
      "Iteration 160, loss = 0.01772538\n",
      "Iteration 161, loss = 0.01761394\n",
      "Iteration 162, loss = 0.01750309\n",
      "Iteration 163, loss = 0.01739282\n",
      "Iteration 164, loss = 0.01728313\n",
      "Iteration 165, loss = 0.01717403\n",
      "Iteration 166, loss = 0.01706550\n",
      "Iteration 167, loss = 0.01695755\n",
      "Iteration 168, loss = 0.01685018\n",
      "Iteration 169, loss = 0.01674338\n",
      "Iteration 170, loss = 0.01663717\n",
      "Iteration 171, loss = 0.01653152\n",
      "Iteration 172, loss = 0.01642646\n",
      "Iteration 173, loss = 0.01632196\n",
      "Iteration 174, loss = 0.01621805\n",
      "Iteration 175, loss = 0.01611470\n",
      "Iteration 176, loss = 0.01601193\n",
      "Iteration 177, loss = 0.01590973\n",
      "Iteration 178, loss = 0.01580810\n",
      "Iteration 179, loss = 0.01570704\n",
      "Iteration 180, loss = 0.01560655\n",
      "Iteration 181, loss = 0.01550663\n",
      "Iteration 182, loss = 0.01540728\n",
      "Iteration 183, loss = 0.01530850\n",
      "Iteration 184, loss = 0.01521028\n",
      "Iteration 185, loss = 0.01511263\n",
      "Iteration 186, loss = 0.01501555\n",
      "Iteration 187, loss = 0.01491903\n",
      "Iteration 188, loss = 0.01482307\n",
      "Iteration 189, loss = 0.01472767\n",
      "Iteration 190, loss = 0.01463284\n",
      "Iteration 191, loss = 0.01453856\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.020000\n",
      "Iteration 192, loss = 0.01444485\n",
      "Iteration 193, loss = 0.01436552\n",
      "Iteration 194, loss = 0.01429252\n",
      "Iteration 195, loss = 0.01422518\n",
      "Iteration 196, loss = 0.01416292\n",
      "Iteration 197, loss = 0.01410523\n",
      "Iteration 198, loss = 0.01405164\n",
      "Iteration 199, loss = 0.01400172\n",
      "Iteration 200, loss = 0.01395511\n",
      "Iteration 201, loss = 0.01391148\n",
      "Iteration 202, loss = 0.01387051\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.004000\n",
      "Iteration 203, loss = 0.01383194\n",
      "Iteration 204, loss = 0.01379819\n",
      "Iteration 205, loss = 0.01376751\n",
      "Iteration 206, loss = 0.01373958\n",
      "Iteration 207, loss = 0.01371412\n",
      "Iteration 208, loss = 0.01369088\n",
      "Iteration 209, loss = 0.01366963\n",
      "Iteration 210, loss = 0.01365018\n",
      "Iteration 211, loss = 0.01363235\n",
      "Iteration 212, loss = 0.01361596\n",
      "Iteration 213, loss = 0.01360087\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000800\n",
      "Iteration 214, loss = 0.01358696\n",
      "Iteration 215, loss = 0.01357463\n",
      "Iteration 216, loss = 0.01356346\n",
      "Iteration 217, loss = 0.01355335\n",
      "Iteration 218, loss = 0.01354418\n",
      "Iteration 219, loss = 0.01353587\n",
      "Iteration 220, loss = 0.01352832\n",
      "Iteration 221, loss = 0.01352145\n",
      "Iteration 222, loss = 0.01351521\n",
      "Iteration 223, loss = 0.01350953\n",
      "Iteration 224, loss = 0.01350434\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000160\n",
      "Iteration 225, loss = 0.01349961\n",
      "Iteration 226, loss = 0.01349538\n",
      "Iteration 227, loss = 0.01349157\n",
      "Iteration 228, loss = 0.01348812\n",
      "Iteration 229, loss = 0.01348501\n",
      "Iteration 230, loss = 0.01348219\n",
      "Iteration 231, loss = 0.01347965\n",
      "Iteration 232, loss = 0.01347734\n",
      "Iteration 233, loss = 0.01347525\n",
      "Iteration 234, loss = 0.01347335\n",
      "Iteration 235, loss = 0.01347164\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000032\n",
      "Iteration 236, loss = 0.01347008\n",
      "Iteration 237, loss = 0.01346868\n",
      "Iteration 238, loss = 0.01346742\n",
      "Iteration 239, loss = 0.01346628\n",
      "Iteration 240, loss = 0.01346526\n",
      "Iteration 241, loss = 0.01346433\n",
      "Iteration 242, loss = 0.01346350\n",
      "Iteration 243, loss = 0.01346274\n",
      "Iteration 244, loss = 0.01346206\n",
      "Iteration 245, loss = 0.01346144\n",
      "Iteration 246, loss = 0.01346088\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000006\n",
      "Iteration 247, loss = 0.01346038\n",
      "Iteration 248, loss = 0.01345993\n",
      "Iteration 249, loss = 0.01345952\n",
      "Iteration 250, loss = 0.01345915\n",
      "Iteration 251, loss = 0.01345882\n",
      "Iteration 252, loss = 0.01345852\n",
      "Iteration 253, loss = 0.01345825\n",
      "Iteration 254, loss = 0.01345801\n",
      "Iteration 255, loss = 0.01345779\n",
      "Iteration 256, loss = 0.01345759\n",
      "Iteration 257, loss = 0.01345741\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000001\n",
      "Iteration 258, loss = 0.01345725\n",
      "Iteration 259, loss = 0.01345711\n",
      "Iteration 260, loss = 0.01345698\n",
      "Iteration 261, loss = 0.01345686\n",
      "Iteration 262, loss = 0.01345675\n",
      "Iteration 263, loss = 0.01345666\n",
      "Iteration 264, loss = 0.01345657\n",
      "Iteration 265, loss = 0.01345649\n",
      "Iteration 266, loss = 0.01345642\n",
      "Iteration 267, loss = 0.01345636\n",
      "Iteration 268, loss = 0.01345630\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
      "Iteration 269, loss = 0.01345625\n",
      "Iteration 270, loss = 0.01345621\n",
      "Iteration 271, loss = 0.01345616\n",
      "Iteration 272, loss = 0.01345613\n",
      "Iteration 273, loss = 0.01345609\n",
      "Iteration 274, loss = 0.01345606\n",
      "Iteration 275, loss = 0.01345604\n",
      "Iteration 276, loss = 0.01345601\n",
      "Iteration 277, loss = 0.01345599\n",
      "Iteration 278, loss = 0.01345597\n",
      "Iteration 279, loss = 0.01345595\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
      "\n",
      "++++++++++  TRAINING:   end  +++++++++++++++\n",
      "The analog prediction error (the loss) is 0.013455950656904896\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "#\n",
    "# Here's where you can change the number of hidden layers\n",
    "# and number of neurons!\n",
    "#\n",
    "nn_classifier = MLPClassifier(hidden_layer_sizes=(6,7),  # 3 input -> 6 -> 7 -> 1 output\n",
    "                    max_iter=500,      # how many times to train\n",
    "                    activation=\"tanh\", # the \"activation function\" input -> output\n",
    "                    solver='sgd',      # the algorithm for optimizing weights\n",
    "                    verbose=True,      # False to \"mute\" the training\n",
    "                    shuffle=True,      # reshuffle the training epochs?\n",
    "                    random_state=None, # set for reproduceability\n",
    "                    learning_rate_init=.1,       # learning rate: % of error to backprop\n",
    "                    learning_rate = 'adaptive')  # soften feedback as it converges\n",
    "\n",
    "# documentation:\n",
    "# scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html \n",
    "#     Try verbose / activation \"relu\" / other network sizes ...\n",
    "\n",
    "print(\"\\n\\n++++++++++  TRAINING:  begin  +++++++++++++++\\n\\n\")\n",
    "nn_classifier.fit(X_train_scaled, y_train_scaled)\n",
    "print(\"\\n++++++++++  TRAINING:   end  +++++++++++++++\")\n",
    "print(f\"The analog prediction error (the loss) is {nn_classifier.loss_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      input  ->  pred   des. \n",
      "           [5.  3.4 1.6 0.4] ->   0      0      correct \n",
      "           [5.6 3.  4.5 1.5] ->   1      1      correct \n",
      "           [5.1 3.8 1.9 0.4] ->   0      0      correct \n",
      "           [6.1 3.  4.9 1.8] ->   2      2      correct \n",
      "           [5.5 2.6 4.4 1.2] ->   1      1      correct \n",
      "           [6.9 3.2 5.7 2.3] ->   2      2      correct \n",
      "           [6.  2.7 5.1 1.6] ->   2      1      incorrect: [1.99713845e-05 9.46095084e-03 9.90519078e-01]\n",
      "           [4.8 3.4 1.6 0.2] ->   0      0      correct \n",
      "           [4.8 3.4 1.9 0.2] ->   0      0      correct \n",
      "           [6.7 3.1 4.4 1.4] ->   1      1      correct \n",
      "           [6.7 3.  5.  1.7] ->   2      1      incorrect: [0.00067016 0.47966322 0.51966663]\n",
      "           [6.4 3.2 4.5 1.5] ->   1      1      correct \n",
      "           [5.6 2.5 3.9 1.1] ->   1      1      correct \n",
      "           [5.8 2.8 5.1 2.4] ->   2      2      correct \n",
      "           [6.8 2.8 4.8 1.4] ->   1      1      correct \n",
      "           [5.1 3.8 1.6 0.2] ->   0      0      correct \n",
      "           [6.7 3.1 5.6 2.4] ->   2      2      correct \n",
      "           [5.9 3.  4.2 1.5] ->   1      1      correct \n",
      "           [5.  3.2 1.2 0.2] ->   0      0      correct \n",
      "           [4.9 3.1 1.5 0.1] ->   0      0      correct \n",
      "           [5.1 3.3 1.7 0.5] ->   0      0      correct \n",
      "           [5.1 3.8 1.5 0.3] ->   0      0      correct \n",
      "           [5.  2.3 3.3 1. ] ->   1      1      correct \n",
      "           [6.3 2.9 5.6 1.8] ->   2      2      correct \n",
      "           [6.4 3.2 5.3 2.3] ->   2      2      correct \n",
      "           [6.  2.2 4.  1. ] ->   1      1      correct \n",
      "           [7.2 3.  5.8 1.6] ->   2      2      correct \n",
      "           [7.  3.2 4.7 1.4] ->   1      1      correct \n",
      "           [5.  3.3 1.4 0.2] ->   0      0      correct \n",
      "\n",
      "correct predictions: 27 out of 29\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# how did it do on the testing data?\n",
    "#\n",
    "\n",
    "def ascii_table_for_classifier(Xsc,y,nn,scaler):\n",
    "    \"\"\" a table including predictions using nn.predict \"\"\"\n",
    "    predictions = nn.predict(Xsc)            # all predictions\n",
    "    prediction_probs = nn.predict_proba(Xsc) # all prediction probabilities\n",
    "    Xpr = scaler.inverse_transform(Xsc)      # Xpr is the \"X to print\": unscaled data!\n",
    "    # count correct\n",
    "    num_correct = 0\n",
    "    # printing\n",
    "    print(f\"{'input ':>28s} -> {'pred':^6s} {'des.':^6s}\") \n",
    "    for i in range(len(y)):\n",
    "        pred = predictions[i]\n",
    "        pred_probs = prediction_probs[i,:]\n",
    "        desired = y[i]\n",
    "        if pred != desired: result = \"  incorrect: \" + str(pred_probs)\n",
    "        else: result = \"  correct\"; num_correct += 1\n",
    "        # Xpr = Xsc  # if you want to see the scaled versions\n",
    "        print(f\"{Xpr[i,0:4]!s:>28s} -> {pred:^6.0f} {desired:^6.0f} {result:^10s}\") \n",
    "    print(f\"\\ncorrect predictions: {num_correct} out of {len(y)}\")\n",
    "    \n",
    "#\n",
    "# let's see how it did on the test data (also the training data!)\n",
    "#\n",
    "ascii_table_for_classifier(X_test_scaled,\n",
    "                           y_test_scaled,\n",
    "                           nn_classifier,\n",
    "                           scaler)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "+++++ parameters, weights, etc. +++++\n",
      "\n",
      "\n",
      "weights/coefficients:\n",
      "\n",
      "[[ 0.71577514 -0.48344458  0.17730161 -0.35870988 -0.27866579  0.16619497]\n",
      " [-0.70299769  0.76693828 -0.64804603  0.27535037 -0.29619662 -0.79179112]\n",
      " [-0.09915524 -0.6149513   1.39178746  0.78250919  2.11679131  0.08053353]\n",
      " [ 0.2839601  -0.72383763  0.03618368  0.68088102  0.96304513 -0.32716529]]\n",
      "[[ 0.21413275  0.40759952 -0.40754848 -0.6094538  -0.34888712 -0.64769912\n",
      "  -0.01035517]\n",
      " [-1.42954781  0.76100632  1.39110104 -1.00552669  0.19150092  0.20295821\n",
      "  -1.41788307]\n",
      " [ 1.0193069  -0.10470335 -0.0250311   0.47995134  0.28145519 -0.19405143\n",
      "   0.58959419]\n",
      " [-0.8281488  -0.79860103 -0.15523871 -0.04235236  0.23887003 -0.19883116\n",
      "  -0.46576591]\n",
      " [-0.02493792 -1.09535164 -1.20028606 -0.4926598   1.89966135 -1.79819018\n",
      "   0.62607378]\n",
      " [-0.44712677  0.12417221 -1.09123117  0.2956209  -0.42985191  0.40451299\n",
      "   0.30032667]]\n",
      "[[-1.15605518  1.02649306  0.365404  ]\n",
      " [ 1.16823532  0.0542765  -1.74412414]\n",
      " [ 1.48290666  0.76529561 -1.66255767]\n",
      " [-1.43988537  0.96966403 -0.63670072]\n",
      " [ 0.02532844 -1.31171694  1.39168937]\n",
      " [ 0.44076003  1.23062122 -1.73375262]\n",
      " [-1.324484    1.4794726   1.26904033]]\n",
      "\n",
      "intercepts: [array([-0.12855803, -0.31207682,  0.4411802 , -0.5240541 , -1.94300509,\n",
      "       -0.53119435]), array([-0.14586593,  0.18021086,  0.36460536, -0.22704159,  0.27336757,\n",
      "        0.60933168,  0.48720882]), array([ 0.40833994, -0.0193382 , -0.02054094])]\n",
      "\n",
      "all parameters: {'activation': 'tanh', 'alpha': 0.0001, 'batch_size': 'auto', 'beta_1': 0.9, 'beta_2': 0.999, 'early_stopping': False, 'epsilon': 1e-08, 'hidden_layer_sizes': (6, 7), 'learning_rate': 'adaptive', 'learning_rate_init': 0.1, 'max_fun': 15000, 'max_iter': 500, 'momentum': 0.9, 'n_iter_no_change': 10, 'nesterovs_momentum': True, 'power_t': 0.5, 'random_state': None, 'shuffle': True, 'solver': 'sgd', 'tol': 0.0001, 'validation_fraction': 0.1, 'verbose': True, 'warm_start': False}\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# We don't usually look inside the NNet, but we can: it's open-box modeling...\n",
    "#\n",
    "if True:  # do we want to see all of the parameters?\n",
    "    nn = nn_classifier  # less to type?\n",
    "    print(\"\\n\\n+++++ parameters, weights, etc. +++++\\n\")\n",
    "    print(f\"\\nweights/coefficients:\\n\")\n",
    "    for wts in nn.coefs_:\n",
    "        print(wts)\n",
    "    print(f\"\\nintercepts: {nn.intercepts_}\")\n",
    "    print(f\"\\nall parameters: {nn.get_params()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input features are [6.7, 3.1, 5.6, 2.4]\n",
      "nn.predict_proba ==  [[2.43335004e-06 2.08463519e-04 9.99789103e-01]]\n",
      "prediction: [2.]\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# we have a predictive model!  Let's try it out...\n",
    "#\n",
    "\n",
    "def make_prediction( Features, nn, scaler ):\n",
    "    \"\"\" uses nn for predictions \"\"\"\n",
    "    print(\"input features are\", Features)\n",
    "    #  we make sure Features has the right shape (list-of-lists)\n",
    "    row = np.array( [Features] )  # makes an array-row\n",
    "    row = scaler.transform(row)   # scale according to scaler\n",
    "    print(\"nn.predict_proba == \", nn.predict_proba(row))   # probabilities of each\n",
    "    prediction = nn.predict(row)  # max!\n",
    "    return prediction\n",
    "    \n",
    "# our features -- note that the inputs don't have to be bits!\n",
    "Features = [ 6.7, 3.1, 5.6, 2.4 ]      # whatever we'd like to test\n",
    "prediction = make_prediction(Features, nn_classifier, scaler)\n",
    "print(f\"prediction: {prediction}\")     # takes the max (nice to see them all!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sepallen': 0, 'sepalwid': 1, 'petallen': 2, 'petalwid': 3, 'irisnum': 4}\n",
      "Index(['sepallen', 'sepalwid', 'petallen', 'petalwid', 'irisnum'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(COL_INDEX)\n",
    "print(COLUMNS)\n",
    "\n",
    "# Let's first predict sepal length ('sepallen', column index 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+++ Start of data-assembly for feature-regression! +++\n",
      "\n",
      "y_all is \n",
      " [4.6 4.3 5.  5.8 4.4 4.4 4.5 4.7 5.  5.4 5.5 4.4 4.6 4.6 4.8 4.8 4.9 5.\n",
      " 5.  5.1 5.1 5.2 5.5 4.6 4.9 4.9 4.9 5.  5.1 5.1 5.1 5.2 5.3 5.4 5.7 4.7\n",
      " 4.8 5.  5.  5.  5.1 5.1 5.4 5.4 5.7 4.8 5.1 7.  4.9 5.  5.  5.7 5.6 5.5\n",
      " 5.5 5.2 5.6 5.8 5.5 5.5 5.8 6.  6.1 5.6 5.8 5.6 5.7 5.9 6.4 5.5 6.3 6.6\n",
      " 6.7 5.4 5.6 5.7 6.  6.  6.2 6.4 6.1 6.5 6.6 6.1 6.1 6.3 6.7 5.9 6.8 6.3\n",
      " 6.9 6.7 6.  4.9 6.  6.2 5.6 6.1 6.3 5.7 6.  6.3 5.8 5.8 5.9 6.3 6.5 6.9\n",
      " 6.5 6.7 6.4 6.4 6.2 6.9 6.4 6.5 6.8 6.1 6.3 6.3 6.4 6.4 6.7 6.7 6.7 6.9\n",
      " 6.5 6.7 7.2 6.8 7.1 7.2 7.2 7.4 7.7 7.3 7.9 7.6 7.7 7.7 7.7]\n",
      "X_all (just features: first few rows) is \n",
      " [[3.6 1.  0.2 0. ]\n",
      " [3.  1.1 0.1 0. ]\n",
      " [3.2 1.2 0.2 0. ]\n",
      " [4.  1.2 0.2 0. ]\n",
      " [3.  1.3 0.2 0. ]]\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Here we set up for a regression model that will predict 'sepallen'  (column index 0)  using\n",
    "#\n",
    "\n",
    "#   sepal width   'sepalwid' (column index 1)\n",
    "#   petal length  'petallen' (column index 2)\n",
    "#   petal width   'petalwid' (column index 3)\n",
    "#     and\n",
    "#   species       'irisnum'  (column index 4)\n",
    "\n",
    "print(\"+++ Start of data-assembly for feature-regression! +++\\n\")\n",
    "# construct the correct X_all from the columns we want\n",
    "# we use np.concatenate to combine parts of the dataset to get all-except-column 0:\n",
    "X_all = np.concatenate( (A[:,0:0], A[:,1:]),axis=1)  # columns 1, 2, 3, and 4\n",
    "\n",
    "# if we wanted all-except-column 1:   X_all = np.concatenate( (A[:,0:1], A[:,2:]),axis=1)  # columns 0, 2, 3, and 4\n",
    "# if we wanted all-except-column 2:   X_all = np.concatenate( (A[:,0:2], A[:,3:]),axis=1)  # columns 0, 1, 3, and 4\n",
    "# if we wanted all-except-column 3:   X_all = np.concatenate( (A[:,0:3], A[:,4:]),axis=1)  # columns 0, 1, 2, and 4\n",
    "# if we wanted all-except-column 4:   X_all = np.concatenate( (A[:,0:4], A[:,5:]),axis=1)  # columns 0, 1, 2, and 3\n",
    "\n",
    "\n",
    "\n",
    "y_all = A[:,0]             # y (labels) ... is all of column 0, sepallen (sepal length)  Re-index, as needed...\n",
    "print(f\"y_all is \\n {y_all}\") \n",
    "print(f\"X_all (just features: first few rows) is \\n {X_all[:5,:]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label-_values_\n",
      " [5.7 5.5 6.1 7.7 6.4 6.3 6.2 6.1 5.6 5.1 6.6 5.7 5.9 6.7 5.  5.2 6.9 5.1\n",
      " 5.3 6.7 7.7 6.3 5.5 4.3 5.  5.5 5.  5.7 5.6 6.1 7.2 6.5 5.8 6.2 6.5 4.5\n",
      " 6.7 5.4 4.6 6.  6.7 5.8 5.5 7.6 7.7 6.  5.4 6.7 5.9 6.8 7.9 4.8 6.3 5.9\n",
      " 5.7 4.8 5.1 6.7 6.4 5.6 7.3 5.5 6.5 6.8 6.3 6.1 5.6 4.8 6.1 6.6 4.9 6.1\n",
      " 5.  6.9 6.8 4.6 4.4 4.7 6.7 5.6 5.5 5.6 6.3 5.  5.  4.7 7.2 5.8 5.8 6.4\n",
      " 7.  6.3 7.1 5.2 6.  6.  5.  6.3 6.  4.9 5.1 6.  5.7 6.4 7.7 4.9 5.  5.7\n",
      " 4.4 7.4 5.8 4.9 5.5 5.2 5.8 6.4 5.4 5.4 6.5 6.3 6.4 7.2 6.5 6.9 4.6 5.1\n",
      " 4.9 6.7 5.  4.6 6.2 4.4 4.9 5.4 5.  4.8 5.1 6.4 5.1 6.9 5.1]\n",
      "\n",
      "features (a few)\n",
      " [[3.8 1.7 0.3 0. ]\n",
      " [4.2 1.4 0.2 0. ]\n",
      " [3.  4.6 1.4 1. ]\n",
      " [2.8 6.7 2.  2. ]\n",
      " [2.8 5.6 2.1 2. ]]\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# we scramble the data, to give a different TRAIN/TEST split each time...\n",
    "# \n",
    "indices = np.random.permutation(len(y_all))  # indices is a permutation-list\n",
    "\n",
    "# we scramble both X and y, necessarily with the same permutation\n",
    "X_all = X_all[indices]              # we apply the _same_ permutation to each!\n",
    "y_all = y_all[indices]              # again...\n",
    "print(\"label-_values_\\n\",y_all)\n",
    "print(\"\\nfeatures (a few)\\n\", X_all[:5,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training with 112 rows;  testing with 29 rows\n",
      "\n",
      "Held-out data... (testing data: 29)\n",
      "y_test: [6.5 6.  6.3 6.7 6.4 6.  4.6 6.  6.3 6.5 6.1 5.5 6.  7.7 6.8 5.  4.9 5.\n",
      " 7.3 7.9 7.4 7.2 5.9 6.4 5.1 7.7 5.  6.8 5.1]\n",
      "\n",
      "X_test (few rows): [[3.  5.5 1.8 2. ]\n",
      " [2.2 5.  1.5 2. ]\n",
      " [2.5 4.9 1.5 1. ]\n",
      " [3.1 5.6 2.4 2. ]\n",
      " [2.8 5.6 2.2 2. ]]\n",
      "\n",
      "Data used for modeling... (training data: 112)\n",
      "y_train: [6.1 5.6 4.8 4.4 6.1 6.  5.7 6.2 5.5 6.4 5.6 5.8 4.4 5.9 5.2 4.9 6.1 4.7\n",
      " 5.8 5.4 6.4 5.1 6.3 5.7 4.4 6.7 4.6 4.3 6.6 5.8 5.6 6.2 6.4 5.2 6.3 4.8\n",
      " 5.8 6.9 6.7 5.  6.5 5.7 5.6 7.1 4.9 5.8 5.6 4.6 7.2 5.5 5.2 6.7 6.9 7.7\n",
      " 7.7 4.5 4.8 5.1 5.7 5.4 6.3 6.6 5.4 4.7 5.5 6.9 6.7 5.1 5.5 5.7 5.5 4.6\n",
      " 4.9 7.2 6.5 7.  6.3 6.5 6.8 5.  6.1 5.3 5.  6.7 5.4 5.6 6.3 5.  6.3 6.2\n",
      " 5.8 5.1 6.4 6.  4.9 5.7 5.5 6.7 5.  4.9 5.4 6.4 5.1 5.  5.  6.7 4.8 6.1\n",
      " 7.6 5.1 5.9 6.9]\n",
      "\n",
      "X_train (few rows): [[2.6 5.6 1.4 2. ]\n",
      " [3.  4.5 1.5 1. ]\n",
      " [3.  1.4 0.1 0. ]\n",
      " [2.9 1.4 0.2 0. ]\n",
      " [2.8 4.  1.3 1. ]]\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# We next separate into test data and training data ... \n",
    "#    + We will train on the training data...\n",
    "#    + We will _not_ look at the testing data to build the model\n",
    "#\n",
    "# Then, afterward, we will test on the testing data -- and see how well we do!\n",
    "#\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_all, y_all, test_size=0.2)\n",
    "\n",
    "print(f\"training with {len(y_train)} rows;  testing with {len(y_test)} rows\\n\" )\n",
    "\n",
    "print(f\"Held-out data... (testing data: {len(y_test)})\")\n",
    "print(f\"y_test: {y_test}\\n\")\n",
    "print(f\"X_test (few rows): {X_test[0:5,:]}\")  # 5 rows\n",
    "print()\n",
    "print(f\"Data used for modeling... (training data: {len(y_train)})\")\n",
    "print(f\"y_train: {y_train}\\n\")\n",
    "print(f\"X_train (few rows): {X_train[0:5,:]}\")  # 5 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    input  -> pred  des. \n",
      "         [-1.05857312  1.14787329  0.32751069  1.27847857] -> ?     6.10 \n",
      "         [-0.15006592  0.51085532  0.45851497  0.05463584] -> ?     5.60 \n",
      "         [-0.15006592 -1.28437714 -1.37554491 -1.1692069 ] -> ?     4.80 \n",
      "         [-0.37719272 -1.28437714 -1.24454064 -1.1692069 ] -> ?     4.40 \n",
      "         [-0.60431952  0.2213017   0.19650642  0.05463584] -> ?     6.10 \n",
      "\n",
      "                                                    input  -> pred  des. \n",
      "                                         [2.6 5.6 1.4 2. ] -> ?     6.10 \n",
      "                                         [3.  4.5 1.5 1. ] -> ?     5.60 \n",
      "[3.00000000e+00 1.40000000e+00 1.00000000e-01 1.11022302e-16] -> ?     4.80 \n",
      "[2.90000000e+00 1.40000000e+00 2.00000000e-01 1.11022302e-16] -> ?     4.40 \n",
      "                                         [2.8 4.  1.3 1. ] -> ?     6.10 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# for NNets, it's important to keep the feature values near 0, say -1. to 1. or so\n",
    "#    This is done through the \"StandardScaler\" in scikit-learn\n",
    "#\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "USE_SCALER = True   # this variable is important! It tracks if we need to use the scaler...\n",
    "\n",
    "# we \"train the scaler\"  (computes the mean and standard deviation)\n",
    "if USE_SCALER == True:\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_train)  # Scale with the training data! ave becomes 0; stdev becomes 1\n",
    "else:\n",
    "    # this one does no scaling!  We still create it to be consistent:\n",
    "    scaler = StandardScaler(copy=True, with_mean=False, with_std=False) # no scaling\n",
    "    scaler.fit(X_train)  # still need to fit, though it does not change...\n",
    "\n",
    "scaler   # is now defined and ready to use...\n",
    "\n",
    "# ++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "\n",
    "# Here are our scaled training and testing sets:\n",
    "\n",
    "X_train_scaled = scaler.transform(X_train) # scale!\n",
    "X_test_scaled = scaler.transform(X_test) # scale!\n",
    "\n",
    "y_train_scaled = y_train  # the predicted/desired labels are not scaled\n",
    "y_test_scaled = y_test  # not using the scaler\n",
    "\n",
    "# reused from above - seeing the scaled data \n",
    "ascii_table(X_train_scaled[0:5,:],y_train_scaled[0:5],None)\n",
    "\n",
    "# reused from above - seeing the unscaled data (inverting the scaler)\n",
    "ascii_table(X_train_scaled[0:5,:],y_train_scaled[0:5],scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "++++++++++  TRAINING:  begin  +++++++++++++++\n",
      "\n",
      "\n",
      "Iteration 1, loss = 21.55772744\n",
      "Iteration 2, loss = 8.95268086\n",
      "Iteration 3, loss = 0.94745943\n",
      "Iteration 4, loss = 1.62428017\n",
      "Iteration 5, loss = 1.26387867\n",
      "Iteration 6, loss = 0.28175406\n",
      "Iteration 7, loss = 0.25895612\n",
      "Iteration 8, loss = 0.22341759\n",
      "Iteration 9, loss = 0.17792478\n",
      "Iteration 10, loss = 0.18699243\n",
      "Iteration 11, loss = 0.19577202\n",
      "Iteration 12, loss = 0.19416643\n",
      "Iteration 13, loss = 0.18489210\n",
      "Iteration 14, loss = 0.17491059\n",
      "Iteration 15, loss = 0.16945270\n",
      "Iteration 16, loss = 0.16584219\n",
      "Iteration 17, loss = 0.16165187\n",
      "Iteration 18, loss = 0.15670549\n",
      "Iteration 19, loss = 0.15108041\n",
      "Iteration 20, loss = 0.14441591\n",
      "Iteration 21, loss = 0.13649299\n",
      "Iteration 22, loss = 0.12754106\n",
      "Iteration 23, loss = 0.11820768\n",
      "Iteration 24, loss = 0.10943492\n",
      "Iteration 25, loss = 0.10223770\n",
      "Iteration 26, loss = 0.09733784\n",
      "Iteration 27, loss = 0.09479870\n",
      "Iteration 28, loss = 0.09394275\n",
      "Iteration 29, loss = 0.09367415\n",
      "Iteration 30, loss = 0.09300144\n",
      "Iteration 31, loss = 0.09141625\n",
      "Iteration 32, loss = 0.08895052\n",
      "Iteration 33, loss = 0.08598696\n",
      "Iteration 34, loss = 0.08300749\n",
      "Iteration 35, loss = 0.08040893\n",
      "Iteration 36, loss = 0.07841182\n",
      "Iteration 37, loss = 0.07704321\n",
      "Iteration 38, loss = 0.07618266\n",
      "Iteration 39, loss = 0.07564813\n",
      "Iteration 40, loss = 0.07526939\n",
      "Iteration 41, loss = 0.07491912\n",
      "Iteration 42, loss = 0.07451581\n",
      "Iteration 43, loss = 0.07401980\n",
      "Iteration 44, loss = 0.07342787\n",
      "Iteration 45, loss = 0.07276350\n",
      "Iteration 46, loss = 0.07206341\n",
      "Iteration 47, loss = 0.07136417\n",
      "Iteration 48, loss = 0.07069268\n",
      "Iteration 49, loss = 0.07006201\n",
      "Iteration 50, loss = 0.06947194\n",
      "Iteration 51, loss = 0.06891291\n",
      "Iteration 52, loss = 0.06837115\n",
      "Iteration 53, loss = 0.06783374\n",
      "Iteration 54, loss = 0.06729213\n",
      "Iteration 55, loss = 0.06674364\n",
      "Iteration 56, loss = 0.06619099\n",
      "Iteration 57, loss = 0.06564046\n",
      "Iteration 58, loss = 0.06509942\n",
      "Iteration 59, loss = 0.06457409\n",
      "Iteration 60, loss = 0.06406802\n",
      "Iteration 61, loss = 0.06358158\n",
      "Iteration 62, loss = 0.06311237\n",
      "Iteration 63, loss = 0.06265623\n",
      "Iteration 64, loss = 0.06220853\n",
      "Iteration 65, loss = 0.06176527\n",
      "Iteration 66, loss = 0.06132374\n",
      "Iteration 67, loss = 0.06088274\n",
      "Iteration 68, loss = 0.06044237\n",
      "Iteration 69, loss = 0.06000369\n",
      "Iteration 70, loss = 0.05956836\n",
      "Iteration 71, loss = 0.05913847\n",
      "Iteration 72, loss = 0.05871644\n",
      "Iteration 73, loss = 0.05830495\n",
      "Iteration 74, loss = 0.05790670\n",
      "Iteration 75, loss = 0.05752415\n",
      "Iteration 76, loss = 0.05715921\n",
      "Iteration 77, loss = 0.05681307\n",
      "Iteration 78, loss = 0.05648611\n",
      "Iteration 79, loss = 0.05617805\n",
      "Iteration 80, loss = 0.05588807\n",
      "Iteration 81, loss = 0.05561500\n",
      "Iteration 82, loss = 0.05535743\n",
      "Iteration 83, loss = 0.05511387\n",
      "Iteration 84, loss = 0.05488278\n",
      "Iteration 85, loss = 0.05466263\n",
      "Iteration 86, loss = 0.05445197\n",
      "Iteration 87, loss = 0.05424947\n",
      "Iteration 88, loss = 0.05405396\n",
      "Iteration 89, loss = 0.05386449\n",
      "Iteration 90, loss = 0.05368031\n",
      "Iteration 91, loss = 0.05350089\n",
      "Iteration 92, loss = 0.05332588\n",
      "Iteration 93, loss = 0.05315510\n",
      "Iteration 94, loss = 0.05298848\n",
      "Iteration 95, loss = 0.05282603\n",
      "Iteration 96, loss = 0.05266780\n",
      "Iteration 97, loss = 0.05251384\n",
      "Iteration 98, loss = 0.05236422\n",
      "Iteration 99, loss = 0.05221891\n",
      "Iteration 100, loss = 0.05207789\n",
      "Iteration 101, loss = 0.05194105\n",
      "Iteration 102, loss = 0.05180826\n",
      "Iteration 103, loss = 0.05167934\n",
      "Iteration 104, loss = 0.05155408\n",
      "Iteration 105, loss = 0.05143225\n",
      "Iteration 106, loss = 0.05131363\n",
      "Iteration 107, loss = 0.05119801\n",
      "Iteration 108, loss = 0.05108518\n",
      "Iteration 109, loss = 0.05097496\n",
      "Iteration 110, loss = 0.05086718\n",
      "Iteration 111, loss = 0.05076172\n",
      "Iteration 112, loss = 0.05065845\n",
      "Iteration 113, loss = 0.05055729\n",
      "Iteration 114, loss = 0.05045815\n",
      "Iteration 115, loss = 0.05036095\n",
      "Iteration 116, loss = 0.05026565\n",
      "Iteration 117, loss = 0.05017217\n",
      "Iteration 118, loss = 0.05008047\n",
      "Iteration 119, loss = 0.04999048\n",
      "Iteration 120, loss = 0.04990216\n",
      "Iteration 121, loss = 0.04981544\n",
      "Iteration 122, loss = 0.04973027\n",
      "Iteration 123, loss = 0.04964659\n",
      "Iteration 124, loss = 0.04956433\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.020000\n",
      "Iteration 125, loss = 0.04948345\n",
      "Iteration 126, loss = 0.04941537\n",
      "Iteration 127, loss = 0.04935317\n",
      "Iteration 128, loss = 0.04929619\n",
      "Iteration 129, loss = 0.04924385\n",
      "Iteration 130, loss = 0.04919564\n",
      "Iteration 131, loss = 0.04915111\n",
      "Iteration 132, loss = 0.04910985\n",
      "Iteration 133, loss = 0.04907151\n",
      "Iteration 134, loss = 0.04903578\n",
      "Iteration 135, loss = 0.04900238\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.004000\n",
      "Iteration 136, loss = 0.04897107\n",
      "Iteration 137, loss = 0.04894372\n",
      "Iteration 138, loss = 0.04891892\n",
      "Iteration 139, loss = 0.04889640\n",
      "Iteration 140, loss = 0.04887592\n",
      "Iteration 141, loss = 0.04885726\n",
      "Iteration 142, loss = 0.04884024\n",
      "Iteration 143, loss = 0.04882468\n",
      "Iteration 144, loss = 0.04881044\n",
      "Iteration 145, loss = 0.04879737\n",
      "Iteration 146, loss = 0.04878536\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000800\n",
      "Iteration 147, loss = 0.04877430\n",
      "Iteration 148, loss = 0.04876450\n",
      "Iteration 149, loss = 0.04875563\n",
      "Iteration 150, loss = 0.04874761\n",
      "Iteration 151, loss = 0.04874034\n",
      "Iteration 152, loss = 0.04873375\n",
      "Iteration 153, loss = 0.04872777\n",
      "Iteration 154, loss = 0.04872234\n",
      "Iteration 155, loss = 0.04871740\n",
      "Iteration 156, loss = 0.04871291\n",
      "Iteration 157, loss = 0.04870881\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000160\n",
      "Iteration 158, loss = 0.04870507\n",
      "Iteration 159, loss = 0.04870174\n",
      "Iteration 160, loss = 0.04869873\n",
      "Iteration 161, loss = 0.04869601\n",
      "Iteration 162, loss = 0.04869355\n",
      "Iteration 163, loss = 0.04869133\n",
      "Iteration 164, loss = 0.04868932\n",
      "Iteration 165, loss = 0.04868750\n",
      "Iteration 166, loss = 0.04868585\n",
      "Iteration 167, loss = 0.04868436\n",
      "Iteration 168, loss = 0.04868300\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000032\n",
      "Iteration 169, loss = 0.04868177\n",
      "Iteration 170, loss = 0.04868067\n",
      "Iteration 171, loss = 0.04867968\n",
      "Iteration 172, loss = 0.04867878\n",
      "Iteration 173, loss = 0.04867798\n",
      "Iteration 174, loss = 0.04867725\n",
      "Iteration 175, loss = 0.04867659\n",
      "Iteration 176, loss = 0.04867599\n",
      "Iteration 177, loss = 0.04867546\n",
      "Iteration 178, loss = 0.04867497\n",
      "Iteration 179, loss = 0.04867453\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000006\n",
      "Iteration 180, loss = 0.04867414\n",
      "Iteration 181, loss = 0.04867378\n",
      "Iteration 182, loss = 0.04867346\n",
      "Iteration 183, loss = 0.04867317\n",
      "Iteration 184, loss = 0.04867291\n",
      "Iteration 185, loss = 0.04867267\n",
      "Iteration 186, loss = 0.04867246\n",
      "Iteration 187, loss = 0.04867227\n",
      "Iteration 188, loss = 0.04867210\n",
      "Iteration 189, loss = 0.04867194\n",
      "Iteration 190, loss = 0.04867180\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000001\n",
      "Iteration 191, loss = 0.04867167\n",
      "Iteration 192, loss = 0.04867156\n",
      "Iteration 193, loss = 0.04867145\n",
      "Iteration 194, loss = 0.04867136\n",
      "Iteration 195, loss = 0.04867128\n",
      "Iteration 196, loss = 0.04867120\n",
      "Iteration 197, loss = 0.04867114\n",
      "Iteration 198, loss = 0.04867107\n",
      "Iteration 199, loss = 0.04867102\n",
      "Iteration 200, loss = 0.04867097\n",
      "++++++++++  TRAINING:   end  +++++++++++++++\n",
      "The (squared) prediction error (the loss) is 0.04867097016991565\n",
      "And, its square root: 0.2206149817440231\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\chung\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# MLPRegressor predicts _floating-point_ outputs\n",
    "#\n",
    "\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "nn_regressor = MLPRegressor(hidden_layer_sizes=(6,7), \n",
    "                    max_iter=200,          # how many training epochs\n",
    "                    activation=\"tanh\",     # the activation function\n",
    "                    solver='sgd',          # the optimizer\n",
    "                    verbose=True,          # do we want to watch as it trains?\n",
    "                    shuffle=True,          # shuffle each epoch?\n",
    "                    random_state=None,     # use for reproducibility\n",
    "                    learning_rate_init=.1, # how much of each error to back-propagate\n",
    "                    learning_rate = 'adaptive')  # how to handle the learning_rate\n",
    "\n",
    "print(\"\\n\\n++++++++++  TRAINING:  begin  +++++++++++++++\\n\\n\")\n",
    "nn_regressor.fit(X_train_scaled, y_train_scaled)\n",
    "print(\"++++++++++  TRAINING:   end  +++++++++++++++\")\n",
    "\n",
    "print(f\"The (squared) prediction error (the loss) is {nn_regressor.loss_}\")\n",
    "print(f\"And, its square root: {nn_regressor.loss_ ** 0.5}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      input  ->   pred    des.    absdiff  \n",
      "[5.20000000e+00 3.50000000e+00 1.50000000e+00 1.11022302e-16] ->  +0.248  +0.200    0.048   \n",
      "           [6.3 2.5 5.  2. ] ->  +1.691  +1.900    0.209   \n",
      "           [6.4 2.9 4.3 1. ] ->  +1.397  +1.300    0.097   \n",
      "           [6.7 3.1 4.7 1. ] ->  +1.506  +1.500    0.006   \n",
      "           [6.2 3.4 5.4 2. ] ->  +2.151  +2.300    0.149   \n",
      "           [6.4 3.2 5.3 2. ] ->  +2.160  +2.300    0.140   \n",
      "           [7.1 3.  5.9 2. ] ->  +2.060  +2.100    0.040   \n",
      "           [5.9 3.  5.1 2. ] ->  +2.090  +1.800    0.290   \n",
      "           [6.3 3.3 4.7 1. ] ->  +1.558  +1.600    0.042   \n",
      "[4.80000000e+00 3.00000000e+00 1.40000000e+00 1.11022302e-16] ->  +0.154  +0.300    0.146   \n",
      "           [5.7 2.6 3.5 1. ] ->  +1.078  +1.000    0.078   \n",
      "[5.70000000e+00 3.80000000e+00 1.70000000e+00 1.11022302e-16] ->  +0.260  +0.300    0.040   \n",
      "           [6.5 3.2 5.1 2. ] ->  +2.133  +2.000    0.133   \n",
      "           [6.  3.  4.8 2. ] ->  +2.060  +1.800    0.260   \n",
      "           [5.6 2.8 4.9 2. ] ->  +1.962  +2.000    0.038   \n",
      "           [7.3 2.9 6.3 2. ] ->  +1.986  +1.800    0.186   \n",
      "           [5.6 3.  4.5 1. ] ->  +1.458  +1.500    0.042   \n",
      "           [6.1 2.8 4.  1. ] ->  +1.288  +1.300    0.012   \n",
      "[5.00000000e+00 3.40000000e+00 1.60000000e+00 1.11022302e-16] ->  +0.235  +0.400    0.165   \n",
      "           [7.7 3.  6.1 2. ] ->  +1.924  +2.300    0.376   \n",
      "           [6.3 3.4 5.6 2. ] ->  +2.182  +2.400    0.218   \n",
      "[5.20000000e+00 3.40000000e+00 1.40000000e+00 1.11022302e-16] ->  +0.225  +0.200    0.025   \n",
      "           [7.7 3.8 6.7 2. ] ->  +2.183  +2.200    0.017   \n",
      "[5.10000000e+00 3.80000000e+00 1.90000000e+00 1.11022302e-16] ->  +0.327  +0.400    0.073   \n",
      "           [7.  3.2 4.7 1. ] ->  +1.462  +1.400    0.062   \n",
      "           [6.1 2.9 4.7 1. ] ->  +1.480  +1.400    0.080   \n",
      "           [6.2 2.2 4.5 1. ] ->  +1.212  +1.500    0.288   \n",
      "           [6.  2.2 5.  2. ] ->  +1.422  +1.500    0.078   \n",
      "[5.10000000e+00 3.30000000e+00 1.70000000e+00 1.11022302e-16] ->  +0.219  +0.500    0.281   \n",
      "\n",
      "+++++   +++++      +++++   +++++   \n",
      "average abs error: 0.12484881830913185\n",
      "+++++   +++++      +++++   +++++   \n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# how did it do? Try out the TEST data...\n",
    "#\n",
    "\n",
    "def ascii_table_for_regressor(Xsc,y,nn,scaler):\n",
    "    \"\"\" a table including predictions using nn.predict \"\"\"\n",
    "    predictions = nn.predict(Xsc) # all predictions\n",
    "    Xpr = scaler.inverse_transform(Xsc)  # Xpr is the \"X to print\": unscaled data!\n",
    "    # measure error\n",
    "    error = 0.0\n",
    "    # printing\n",
    "    print(f\"{'input ':>28s} ->  {'pred':^6s}  {'des.':^6s}  {'absdiff':^10s}\") \n",
    "    for i in range(len(y)):\n",
    "        pred = predictions[i]\n",
    "        desired = y[i]\n",
    "        result = abs(desired - pred)\n",
    "        error += result\n",
    "        # Xpr = Xsc   # if you'd like to see the scaled values\n",
    "        print(f\"{Xpr[i,0:4]!s:>28s} ->  {pred:<+6.3f}  {desired:<+6.3f}  {result:^10.3f}\") \n",
    "\n",
    "    print(\"\\n\" + \"+++++   +++++      +++++   +++++   \")\n",
    "    print(f\"average abs error: {error/len(y)}\")\n",
    "    print(\"+++++   +++++      +++++   +++++   \")\n",
    "    \n",
    "#\n",
    "# let's see how it did on the test data (also the training data!)\n",
    "#\n",
    "ascii_table_for_regressor(X_test_scaled,\n",
    "                          y_test_scaled,\n",
    "                          nn_regressor,\n",
    "                          scaler)   # this is our own f'n, above\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+++ Start of data-assembly for feature-regression! +++\n",
      "\n",
      "average abs error: 0.2523899376242552 for iteration 0\n",
      "average abs error: 0.20686995917477938 for iteration 1\n",
      "average abs error: 0.33074590490155203 for iteration 2\n",
      "average abs error: 0.14797933053053164 for iteration 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\chung\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "print(\"+++ Start of data-assembly for feature-regression! +++\\n\")\n",
    "# construct the correct X_all from the columns we want\n",
    "# we use np.concatenate to combine parts of the dataset to get all-except-column 0:\n",
    "\n",
    "for f in range(4):\n",
    "    X_all = np.concatenate( (A[:,0:f], A[:,1+f:]),axis=1) \n",
    "    y_all = A[:,f+1]             # y (labels) ... is all of column 0, sepallen (sepal length)  Re-index, as needed...\n",
    "    # \n",
    "   \n",
    "    # if we wanted all-except-column 1:   X_all = np.concatenate( (A[:,0:1], A[:,2:]),axis=1)  # columns 0, 2, 3, and 4\n",
    "    # if we wanted all-except-column 2:   X_all = np.concatenate( (A[:,0:2], A[:,3:]),axis=1)  # columns 0, 1, 3, and 4\n",
    "    # if we wanted all-except-column 3:   X_all = np.concatenate( (A[:,0:3], A[:,4:]),axis=1)  # columns 0, 1, 2, and 4\n",
    "    # if we wanted all-except-column 4:   X_all = np.concatenate( (A[:,0:4], A[:,5:]),axis=1)  # columns 0, 1, 2, and 3\n",
    "    # slicing is forgiving...\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    # we scramble the data, to give a different TRAIN/TEST split each time...\n",
    "    # \n",
    "    indices = np.random.permutation(len(y_all))  # indices is a permutation-list\n",
    "\n",
    "    # we scramble both X and y, necessarily with the same permutation\n",
    "    X_all = X_all[indices]              # we apply the _same_ permutation to each!\n",
    "    y_all = y_all[indices]              # again...\n",
    "\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_all, y_all, test_size=0.2)\n",
    "\n",
    "    # Scaling Part...\n",
    "\n",
    "    USE_SCALER = True   # this variable is important! It tracks if we need to use the scaler...\n",
    "\n",
    "    # we \"train the scaler\"  (computes the mean and standard deviation)\n",
    "    if USE_SCALER == True:\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(X_train)  # Scale with the training data! ave becomes 0; stdev becomes 1\n",
    "    else:\n",
    "        # this one does no scaling!  We still create it to be consistent:\n",
    "        scaler = StandardScaler(copy=True, with_mean=False, with_std=False) # no scaling\n",
    "        scaler.fit(X_train)  # still need to fit, though it does not change...\n",
    "\n",
    "    # ++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "\n",
    "    # Here are our scaled training and testing sets:\n",
    "\n",
    "    X_train_scaled = scaler.transform(X_train) # scale!\n",
    "    X_test_scaled = scaler.transform(X_test) # scale!\n",
    "\n",
    "    y_train_scaled = y_train  # the predicted/desired labels are not scaled\n",
    "    y_test_scaled = y_test  # not using the scaler\n",
    "\n",
    "    #\n",
    "    # MLPRegressor predicts _floating-point_ outputs\n",
    "    #\n",
    "\n",
    "\n",
    "    nn_regressor = MLPRegressor(hidden_layer_sizes=(6,7), \n",
    "                        max_iter=200,          # how many training epochs\n",
    "                        activation=\"tanh\",     # the activation function\n",
    "                        solver='sgd',          # the optimizer\n",
    "                        verbose=False,          # do we want to watch as it trains?\n",
    "                        shuffle=True,          # shuffle each epoch?\n",
    "                        random_state=None,     # use for reproducibility\n",
    "                        learning_rate_init=.1, # how much of each error to back-propagate\n",
    "                        learning_rate = 'adaptive')  # how to handle the learning_rate\n",
    "\n",
    "    # print(\"\\n\\n++++++++++  TRAINING:  begin  +++++++++++++++\\n\\n\")\n",
    "    nn_regressor.fit(X_train_scaled, y_train_scaled)\n",
    "    # print(\"++++++++++  TRAINING:   end  +++++++++++++++\")\n",
    "\n",
    "    # print(f\"The (squared) prediction error (the loss) is {nn_regressor.loss_}\")\n",
    "    # print(f\"And, its square root: {nn_regressor.loss_ ** 0.5}\")\n",
    "    # print()\n",
    "    # print()\n",
    "#\n",
    "# how did it do? Try out the TEST data...\n",
    "#\n",
    "    predictions = nn_regressor.predict(X_test_scaled) # all predictions\n",
    "    Xpr = scaler.inverse_transform(X_test_scaled)  # Xpr is the \"X to print\": unscaled data!\n",
    "    # measure error\n",
    "    error = 0.0\n",
    "    # printing\n",
    "    for i in range(len(y_test_scaled)):\n",
    "        pred = predictions[i]\n",
    "        desired = y_test_scaled[i]\n",
    "        result = abs(desired - pred)\n",
    "        error += result\n",
    "        # Xpr = Xsc   # if you'd like to see the scaled values\n",
    "\n",
    "    print(f\"average abs error: {error/len(y_test_scaled)} for iteration {f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "1f0c8735478a48ff7ef3deb8c421f15aa5d573c59a98bc92eb9b829f28c47b33"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
