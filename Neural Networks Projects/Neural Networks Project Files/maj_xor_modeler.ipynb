{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "####  Modeling with both proximity and conditionals:  Neural Nets!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#   We demonstrate _both_ clasification + regression for bitwise functions:\n",
    "\n",
    "#     + function #1:  MAJ, the \"majority\" function\n",
    "#                     three bits input, the most-appearing bit is the output \n",
    "\n",
    "#     + function #2:  XOR, the \"xor\" or \"odd # of 1's\" function \n",
    "#                     three bits input, output is their sum%2 \n",
    "#                     that is, 1 if there is an odd # of 1's, 0 if an even # of 1's\n",
    "#   \n",
    "#   From here, we'll use NNets for the births and iris datasets\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries!\n",
    "import numpy as np      # numpy is Python's \"array\" library\n",
    "import pandas as pd     # Pandas is Python's \"data\" library (\"dataframe\" == spreadsheet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xor_cleaned.csv : file read into a pandas dataframe.\n"
     ]
    }
   ],
   "source": [
    "# let's read in our data...\n",
    "# \n",
    "# for read_csv, use header=0 when row 0 is a header row\n",
    "# \n",
    "filename = 'xor_cleaned.csv'\n",
    "# filename = 'maj_cleaned.csv'\n",
    "df = pd.read_csv(filename, header=0)   # encoding=\"latin1\" et al.\n",
    "print(f\"{filename} : file read into a pandas dataframe.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8 entries, 0 to 7\n",
      "Data columns (total 4 columns):\n",
      " #   Column     Non-Null Count  Dtype\n",
      "---  ------     --------------  -----\n",
      " 0   bit1       8 non-null      int64\n",
      " 1   bit2       8 non-null      int64\n",
      " 2   bit3       8 non-null      int64\n",
      " 3   outputbit  8 non-null      int64\n",
      "dtypes: int64(4)\n",
      "memory usage: 384.0 bytes\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# let's look at our pandas dataframe  \n",
    "#\n",
    "df.info()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COLUMNS is Index(['bit1', 'bit2', 'bit3', 'outputbit'], dtype='object')\n",
      "\n",
      "COLUMNS[0] is bit1\n",
      "\n",
      "COL_INDEX is {'bit1': 0, 'bit2': 1, 'bit3': 2, 'outputbit': 3}\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# let's keep our column names in variables, for reference\n",
    "#\n",
    "COLUMNS = df.columns            # \"list\" of columns\n",
    "print(f\"COLUMNS is {COLUMNS}\\n\")  \n",
    "  # It's a \"pandas\" list, called an Index\n",
    "  # use it just as a Python list of strings:\n",
    "print(f\"COLUMNS[0] is {COLUMNS[0]}\\n\")\n",
    "\n",
    "# let's create a dictionary to look up any column index by name\n",
    "COL_INDEX = {}\n",
    "for i, name in enumerate(COLUMNS):\n",
    "    COL_INDEX[name] = i  # using the name (as key), look up the value (i)\n",
    "print(f\"COL_INDEX is {COL_INDEX}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zero maps to 0\n",
      "one maps to 1\n"
     ]
    }
   ],
   "source": [
    "# all of scikit-learn's ML routines need numbers, not strings\n",
    "#   ... even for categories/classifications (like species!)\n",
    "#   so, we will convert the flower-species to numbers:\n",
    "\n",
    "SPECIES = ['zero','one']   # int to str\n",
    "SPECIES_INDEX = {'zero':0,'one':1}  # str to int\n",
    "\n",
    "def convert_species(speciesname):\n",
    "    \"\"\" return the species index (a unique integer/category) \"\"\"\n",
    "    #print(f\"converting {speciesname}...\")\n",
    "    return SPECIES_INDEX[speciesname]\n",
    "\n",
    "# Let's try it out...\n",
    "for name in SPECIES:\n",
    "    print(f\"{name} maps to {convert_species(name)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 1. 1.]\n",
      " [0. 1. 0. 1.]\n",
      " [0. 1. 1. 0.]\n",
      " [1. 0. 0. 1.]\n",
      " [1. 0. 1. 0.]\n",
      " [1. 1. 0. 0.]\n",
      " [1. 1. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# let's convert our dataframe to a numpy array, named A\n",
    "#    Our ML library, scikit-learn operates entirely on numpy arrays.\n",
    "#\n",
    "A = df.to_numpy()    \n",
    "A = A.astype('float64')   # and make things floating-point\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 1. 1.]\n",
      " [0. 1. 0. 1.]\n",
      " [0. 1. 1. 0.]\n",
      " [1. 0. 0. 1.]\n",
      " [1. 0. 1. 0.]\n",
      " [1. 1. 0. 0.]\n",
      " [1. 1. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Let's make sure things are floats! (Important for NNets!)\n",
    "# \n",
    "\n",
    "A = A.astype('float64')   # and make things floating-point\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            input  -> pred  des. \n",
      "        [0. 0. 0.] -> ?     0    \n",
      "        [0. 0. 1.] -> ?     1    \n",
      "        [0. 1. 0.] -> ?     1    \n",
      "        [0. 1. 1.] -> ?     0    \n",
      "        [1. 0. 0.] -> ?     1    \n",
      "        [1. 0. 1.] -> ?     0    \n",
      "        [1. 1. 0.] -> ?     0    \n",
      "        [1. 1. 1.] -> ?     1    \n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Let's split into features and labels (species/categories):\n",
    "#\n",
    "\n",
    "X_all = A[:,0:3].copy()   # We make a copy so we don't change A\n",
    "y_all = A[:,3].copy()\n",
    "\n",
    "def ascii_table(X,y):\n",
    "    \"\"\" print a table of binary inputs and outputs \"\"\"\n",
    "    print(f\"{'input ':>18s} -> {'pred':<5s} {'des.':<5s}\") \n",
    "    for i in range(len(y)):\n",
    "        print(f\"{X[i,:]!s:>18s} -> {'?':<5s} {y[i]:<5.0f}\")   # !s is str ...\n",
    "        \n",
    "ascii_table(X_all,y_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            input  -> pred  des. \n",
      "        [0. 0. 0.] -> ?     0    \n",
      "        [0. 0. 1.] -> ?     1    \n",
      "        [0. 1. 0.] -> ?     1    \n",
      "        [0. 1. 1.] -> ?     0    \n",
      "        [1. 0. 0.] -> ?     1    \n",
      "        [1. 0. 1.] -> ?     0    \n",
      "        [1. 1. 0.] -> ?     0    \n",
      "        [1. 1. 1.] -> ?     1    \n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# we can scramble the remaining data if we want to...\n",
    "#    we don't want to, at the moment...\n",
    "#\n",
    "\n",
    "SCRAMBLE = False   # easy to change...\n",
    "\n",
    "if SCRAMBLE == True:\n",
    "    NUM_ROWS = len(y_def)\n",
    "    indices = np.random.permutation(NUM_ROWS)  # this scrambles the data each time\n",
    "    X_all = X_all[indices]\n",
    "    y_all = y_all[indices]\n",
    "else:\n",
    "    X_all = X_all  # don't scramble\n",
    "    y_all = y_all\n",
    "\n",
    "ascii_table(X_all,y_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            input  -> pred  des. \n",
      "        [0. 0. 0.] -> ?     0    \n",
      "        [0. 0. 1.] -> ?     1    \n",
      "        [0. 1. 0.] -> ?     1    \n",
      "        [0. 1. 1.] -> ?     0    \n",
      "        [1. 0. 0.] -> ?     1    \n",
      "        [1. 0. 1.] -> ?     0    \n",
      "        [1. 1. 0.] -> ?     0    \n",
      "        [1. 1. 1.] -> ?     1    \n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# here, we _cheat_ by letting the full dataset \n",
    "# be _both_ the training and testing sets.  (There are too few otherwise!)\n",
    "#\n",
    "X_train = X_all.copy()\n",
    "y_train = y_all.copy()\n",
    "\n",
    "X_test = X_all.copy()\n",
    "y_test = y_all.copy()\n",
    "\n",
    "ascii_table(X_train,y_train)    # same as above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            input  -> pred  des. \n",
      "     [-1. -1. -1.] -> ?     0    \n",
      "     [-1. -1.  1.] -> ?     1    \n",
      "     [-1.  1. -1.] -> ?     1    \n",
      "     [-1.  1.  1.] -> ?     0    \n",
      "     [ 1. -1. -1.] -> ?     1    \n",
      "     [ 1. -1.  1.] -> ?     0    \n",
      "     [ 1.  1. -1.] -> ?     0    \n",
      "        [1. 1. 1.] -> ?     1    \n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# for NNets, it's important to keep the feature values near 0, say -1. to 1. or so\n",
    "#    This is done through the \"StandardScaler\" in scikit-learn\n",
    "# \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "USE_SCALER = True   # this variable is important! It tracks if we need to use the scaler...\n",
    "\n",
    "# we \"train the scaler\"  (computes the mean and standard deviation)\n",
    "if USE_SCALER == True:\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_train)  # Scale with the training data! ave becomes 0; stdev becomes 1\n",
    "else:\n",
    "    # this one does no scaling!  \n",
    "    #\n",
    "    # Its inputs are saying \"don't find the mean, don't use the st deviation...\"\n",
    "    #\n",
    "    # We still create it to be consistent...\n",
    "    scaler = StandardScaler(copy=True, with_mean=False, with_std=False)\n",
    "    scaler.fit(X_train)  # still need to fit, though it does not change...\n",
    "\n",
    "scaler   # is now defined and ready to use...\n",
    "\n",
    "# ++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "\n",
    "# Here are our scaled training and testing sets:\n",
    "\n",
    "X_train_scaled = scaler.transform(X_train) # scale!\n",
    "X_test_scaled = scaler.transform(X_test) # scale!\n",
    "\n",
    "y_train_scaled = y_train  # the predicted/desired labels are not scaled\n",
    "y_test_scaled = y_test  # not using the scaler\n",
    "    \n",
    "ascii_table(X_train_scaled,y_train_scaled)\n",
    "\n",
    "#\n",
    "# Note that the zeros have become -1's\n",
    "# and the 1's have stayed 1's\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "++++++++++  TRAINING:  begin  +++++++++++++++\n",
      "\n",
      "\n",
      "Iteration 1, loss = 0.80358163\n",
      "Iteration 2, loss = 0.74313317\n",
      "Iteration 3, loss = 0.68564791\n",
      "Iteration 4, loss = 0.64628144\n",
      "Iteration 5, loss = 0.62693421\n",
      "Iteration 6, loss = 0.61899236\n",
      "Iteration 7, loss = 0.61152413\n",
      "Iteration 8, loss = 0.59750454\n",
      "Iteration 9, loss = 0.57530704\n",
      "Iteration 10, loss = 0.54724162\n",
      "Iteration 11, loss = 0.51702695\n",
      "Iteration 12, loss = 0.48740955\n",
      "Iteration 13, loss = 0.45900222\n",
      "Iteration 14, loss = 0.43070689\n",
      "Iteration 15, loss = 0.40102397\n",
      "Iteration 16, loss = 0.36916273\n",
      "Iteration 17, loss = 0.33532431\n",
      "Iteration 18, loss = 0.30030252\n",
      "Iteration 19, loss = 0.26498150\n",
      "Iteration 20, loss = 0.23014835\n",
      "Iteration 21, loss = 0.19661043\n",
      "Iteration 22, loss = 0.16531936\n",
      "Iteration 23, loss = 0.13723116\n",
      "Iteration 24, loss = 0.11297793\n",
      "Iteration 25, loss = 0.09268563\n",
      "Iteration 26, loss = 0.07606552\n",
      "Iteration 27, loss = 0.06262039\n",
      "Iteration 28, loss = 0.05180967\n",
      "Iteration 29, loss = 0.04313583\n",
      "Iteration 30, loss = 0.03617481\n",
      "Iteration 31, loss = 0.03057831\n",
      "Iteration 32, loss = 0.02606559\n",
      "Iteration 33, loss = 0.02241261\n",
      "Iteration 34, loss = 0.01944178\n",
      "Iteration 35, loss = 0.01701288\n",
      "Iteration 36, loss = 0.01501559\n",
      "Iteration 37, loss = 0.01336314\n",
      "Iteration 38, loss = 0.01198739\n",
      "Iteration 39, loss = 0.01083466\n",
      "Iteration 40, loss = 0.00986266\n",
      "Iteration 41, loss = 0.00903791\n",
      "Iteration 42, loss = 0.00833380\n",
      "Iteration 43, loss = 0.00772911\n",
      "Iteration 44, loss = 0.00720682\n",
      "Iteration 45, loss = 0.00675322\n",
      "Iteration 46, loss = 0.00635718\n",
      "Iteration 47, loss = 0.00600964\n",
      "Iteration 48, loss = 0.00570320\n",
      "Iteration 49, loss = 0.00543174\n",
      "Iteration 50, loss = 0.00519019\n",
      "Iteration 51, loss = 0.00497435\n",
      "Iteration 52, loss = 0.00478070\n",
      "Iteration 53, loss = 0.00460628\n",
      "Iteration 54, loss = 0.00444860\n",
      "Iteration 55, loss = 0.00430552\n",
      "Iteration 56, loss = 0.00417525\n",
      "Iteration 57, loss = 0.00405625\n",
      "Iteration 58, loss = 0.00394718\n",
      "Iteration 59, loss = 0.00384691\n",
      "Iteration 60, loss = 0.00375445\n",
      "Iteration 61, loss = 0.00366893\n",
      "Iteration 62, loss = 0.00358962\n",
      "Iteration 63, loss = 0.00351586\n",
      "Iteration 64, loss = 0.00344708\n",
      "Iteration 65, loss = 0.00338277\n",
      "Iteration 66, loss = 0.00332250\n",
      "Iteration 67, loss = 0.00326588\n",
      "Iteration 68, loss = 0.00321255\n",
      "Iteration 69, loss = 0.00316222\n",
      "Iteration 70, loss = 0.00311462\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.020000\n",
      "Iteration 71, loss = 0.00306949\n",
      "Iteration 72, loss = 0.00303103\n",
      "Iteration 73, loss = 0.00299631\n",
      "Iteration 74, loss = 0.00296489\n",
      "Iteration 75, loss = 0.00293638\n",
      "Iteration 76, loss = 0.00291046\n",
      "Iteration 77, loss = 0.00288683\n",
      "Iteration 78, loss = 0.00286524\n",
      "Iteration 79, loss = 0.00284546\n",
      "Iteration 80, loss = 0.00282730\n",
      "Iteration 81, loss = 0.00281058\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.004000\n",
      "Iteration 82, loss = 0.00279514\n",
      "Iteration 83, loss = 0.00278158\n",
      "Iteration 84, loss = 0.00276934\n",
      "Iteration 85, loss = 0.00275829\n",
      "Iteration 86, loss = 0.00274829\n",
      "Iteration 87, loss = 0.00273923\n",
      "Iteration 88, loss = 0.00273101\n",
      "Iteration 89, loss = 0.00272355\n",
      "Iteration 90, loss = 0.00271677\n",
      "Iteration 91, loss = 0.00271059\n",
      "Iteration 92, loss = 0.00270495\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000800\n",
      "Iteration 93, loss = 0.00269980\n",
      "Iteration 94, loss = 0.00269522\n",
      "Iteration 95, loss = 0.00269109\n",
      "Iteration 96, loss = 0.00268736\n",
      "Iteration 97, loss = 0.00268399\n",
      "Iteration 98, loss = 0.00268094\n",
      "Iteration 99, loss = 0.00267818\n",
      "Iteration 100, loss = 0.00267569\n",
      "Iteration 101, loss = 0.00267343\n",
      "Iteration 102, loss = 0.00267138\n",
      "Iteration 103, loss = 0.00266952\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000160\n",
      "Iteration 104, loss = 0.00266782\n",
      "Iteration 105, loss = 0.00266631\n",
      "Iteration 106, loss = 0.00266495\n",
      "Iteration 107, loss = 0.00266372\n",
      "Iteration 108, loss = 0.00266261\n",
      "Iteration 109, loss = 0.00266161\n",
      "Iteration 110, loss = 0.00266070\n",
      "Iteration 111, loss = 0.00265989\n",
      "Iteration 112, loss = 0.00265915\n",
      "Iteration 113, loss = 0.00265848\n",
      "Iteration 114, loss = 0.00265787\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000032\n",
      "Iteration 115, loss = 0.00265733\n",
      "Iteration 116, loss = 0.00265684\n",
      "Iteration 117, loss = 0.00265639\n",
      "Iteration 118, loss = 0.00265600\n",
      "Iteration 119, loss = 0.00265564\n",
      "Iteration 120, loss = 0.00265531\n",
      "Iteration 121, loss = 0.00265502\n",
      "Iteration 122, loss = 0.00265476\n",
      "Iteration 123, loss = 0.00265452\n",
      "Iteration 124, loss = 0.00265430\n",
      "Iteration 125, loss = 0.00265411\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000006\n",
      "Iteration 126, loss = 0.00265393\n",
      "Iteration 127, loss = 0.00265378\n",
      "Iteration 128, loss = 0.00265363\n",
      "Iteration 129, loss = 0.00265351\n",
      "Iteration 130, loss = 0.00265339\n",
      "Iteration 131, loss = 0.00265329\n",
      "Iteration 132, loss = 0.00265319\n",
      "Iteration 133, loss = 0.00265311\n",
      "Iteration 134, loss = 0.00265303\n",
      "Iteration 135, loss = 0.00265296\n",
      "Iteration 136, loss = 0.00265290\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000001\n",
      "Iteration 137, loss = 0.00265285\n",
      "Iteration 138, loss = 0.00265280\n",
      "Iteration 139, loss = 0.00265275\n",
      "Iteration 140, loss = 0.00265271\n",
      "Iteration 141, loss = 0.00265267\n",
      "Iteration 142, loss = 0.00265264\n",
      "Iteration 143, loss = 0.00265261\n",
      "Iteration 144, loss = 0.00265259\n",
      "Iteration 145, loss = 0.00265256\n",
      "Iteration 146, loss = 0.00265254\n",
      "Iteration 147, loss = 0.00265252\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
      "Iteration 148, loss = 0.00265250\n",
      "Iteration 149, loss = 0.00265249\n",
      "Iteration 150, loss = 0.00265247\n",
      "Iteration 151, loss = 0.00265246\n",
      "Iteration 152, loss = 0.00265245\n",
      "Iteration 153, loss = 0.00265244\n",
      "Iteration 154, loss = 0.00265243\n",
      "Iteration 155, loss = 0.00265242\n",
      "Iteration 156, loss = 0.00265241\n",
      "Iteration 157, loss = 0.00265240\n",
      "Iteration 158, loss = 0.00265240\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
      "\n",
      "++++++++++  TRAINING:   end  +++++++++++++++\n",
      "The analog prediction error (the loss) is 0.002652398718875501\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# import our NNet library (within scikit-learn)\n",
    "#\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "#\n",
    "# Here's where you can change the number of layers, neurons, and other parameters:\n",
    "#\n",
    "nn_classifier = MLPClassifier(hidden_layer_sizes=(6,7),  # 3 input -> 6 -> 7 -> 1 output\n",
    "                    max_iter=500,      # how many times to train\n",
    "                    activation=\"tanh\", # the \"activation function\" input -> output\n",
    "                    solver='sgd',      # the algorithm for optimizing weights\n",
    "                    verbose=True,      # False to \"mute\" the training\n",
    "                    shuffle=True,      # reshuffle the training epochs?\n",
    "                    random_state=None, # set for reproduceability\n",
    "                    learning_rate_init=.1,       # learning rate: % of error to backprop\n",
    "                    learning_rate = 'adaptive')  # soften feedback as it converges\n",
    "\n",
    "# documentation:\n",
    "# scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html \n",
    "#     Try verbose / activation \"relu\" / other network sizes ...\n",
    "\n",
    "print(\"\\n\\n++++++++++  TRAINING:  begin  +++++++++++++++\\n\\n\")\n",
    "nn_classifier.fit(X_train_scaled, y_train_scaled)\n",
    "print(\"\\n++++++++++  TRAINING:   end  +++++++++++++++\")\n",
    "print(f\"The analog prediction error (the loss) is {nn_classifier.loss_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            input  ->  pred   des. \n",
      "        [0. 0. 0.] ->   0      0      correct \n",
      "        [0. 0. 1.] ->   1      1      correct \n",
      "        [0. 1. 0.] ->   1      1      correct \n",
      "        [0. 1. 1.] ->   0      0      correct \n",
      "        [1. 0. 0.] ->   1      1      correct \n",
      "        [1. 0. 1.] ->   0      0      correct \n",
      "        [1. 1. 0.] ->   0      0      correct \n",
      "        [1. 1. 1.] ->   1      1      correct \n",
      "\n",
      "correct predictions: 8 out of 8\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# how did it do on the data?    (Here, remember, training == testing: we're cheating!)   \n",
    "#\n",
    "\n",
    "def ascii_table_for_classifier(Xsc,y,nn,scaler):\n",
    "    \"\"\" a table including predictions using nn.predict \"\"\"\n",
    "    predictions = nn.predict(Xsc)            # all predictions\n",
    "    prediction_probs = nn.predict_proba(Xsc) # all prediction probabilities\n",
    "    Xpr = scaler.inverse_transform(Xsc)      # Xpr is the \"X to print\": unscaled data!\n",
    "    # count correct\n",
    "    num_correct = 0\n",
    "    # printing\n",
    "    print(f\"{'input ':>18s} -> {'pred':^6s} {'des.':^6s}\") \n",
    "    for i in range(len(y)):\n",
    "        pred = predictions[i]\n",
    "        pred_probs = prediction_probs[i,:]\n",
    "        desired = y[i]\n",
    "        if pred != desired: result = \"  incorrect: \" + str(pred_probs)\n",
    "        else: result = \"  correct\"; num_correct += 1\n",
    "        # Xpr = Xsc  # if you want to see the scaled versions\n",
    "        print(f\"{Xpr[i,:]!s:>18s} -> {pred:^6.0f} {desired:^6.0f} {result:^10s}\") \n",
    "    print(f\"\\ncorrect predictions: {num_correct} out of {len(y)}\")\n",
    "    \n",
    "#\n",
    "# let's see how it did on the test data (also the training data!)\n",
    "#\n",
    "ascii_table_for_classifier(X_test_scaled,\n",
    "                           y_test_scaled,\n",
    "                           nn_classifier,\n",
    "                           scaler)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "+++++ parameters, weights, etc. +++++\n",
      "\n",
      "\n",
      "weights/coefficients:\n",
      "\n",
      "[[ 1.21382934 -1.53652386  1.34796333 -0.5727733  -1.15148375 -0.86964985]\n",
      " [ 1.52802173  1.7472659  -0.03630473 -0.92608976 -0.24494094 -0.31015794]\n",
      " [-1.39840348 -1.84887153 -0.15291766  0.78774776 -0.15461596 -0.1718161 ]]\n",
      "[[ 0.58721342  1.26616396  0.15941477 -0.19511234  1.39773185 -0.44373931\n",
      "  -0.72713351]\n",
      " [-0.99518225 -1.01183639 -0.91222726  0.22893017 -1.59768739  0.46979773\n",
      "   0.75892451]\n",
      " [ 0.17829857 -0.86614084 -0.97028741 -0.67555495 -0.83876894  0.53608387\n",
      "   0.12439031]\n",
      " [ 0.13516531  0.0488237  -0.96040747 -0.1290118  -0.49391318  0.24074678\n",
      "   0.38373951]\n",
      " [ 0.03807796  0.27267073 -0.13207998 -0.2187538   0.69205944 -0.2272655\n",
      "  -0.54530881]\n",
      " [ 0.79100444  0.27032176  0.29656529 -0.1658389   0.6867491  -0.09457431\n",
      "   0.25564488]]\n",
      "[[ 1.1514851 ]\n",
      " [ 1.93992299]\n",
      " [ 1.28994139]\n",
      " [-0.10097596]\n",
      " [ 2.75222561]\n",
      " [-0.91049168]\n",
      " [-1.06650915]]\n",
      "\n",
      "intercepts: [array([ 0.01208356,  0.01901037, -0.41008127, -0.13167607, -0.5923037 ,\n",
      "        0.46351982]), array([ 0.08836834, -0.18752036, -0.25118968, -0.58717843, -0.13704264,\n",
      "        0.03709437, -0.00772587]), array([0.0063965])]\n",
      "\n",
      "all parameters: {'activation': 'tanh', 'alpha': 0.0001, 'batch_size': 'auto', 'beta_1': 0.9, 'beta_2': 0.999, 'early_stopping': False, 'epsilon': 1e-08, 'hidden_layer_sizes': (6, 7), 'learning_rate': 'adaptive', 'learning_rate_init': 0.1, 'max_fun': 15000, 'max_iter': 500, 'momentum': 0.9, 'n_iter_no_change': 10, 'nesterovs_momentum': True, 'power_t': 0.5, 'random_state': None, 'shuffle': True, 'solver': 'sgd', 'tol': 0.0001, 'validation_fraction': 0.1, 'verbose': True, 'warm_start': False}\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# More rarely, we will want to see the neurons' weights and other details:\n",
    "#\n",
    "\n",
    "nn = nn_classifier  # less to type...\n",
    "print(\"\\n\\n+++++ parameters, weights, etc. +++++\\n\")\n",
    "print(f\"\\nweights/coefficients:\\n\")\n",
    "for wts in nn.coefs_:\n",
    "    print(wts)\n",
    "print(f\"\\nintercepts: {nn.intercepts_}\")\n",
    "print(f\"\\nall parameters: {nn.get_params()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input features are [1, 0, 1]\n",
      "nn.predict_proba ==  [[0.99660997 0.00339003]]\n",
      "prediction: [0.]\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# we have a predictive model!  Let's try it out...\n",
    "#\n",
    "def make_prediction( Features, nn, scaler ):\n",
    "    \"\"\" uses nn for predictions \"\"\"\n",
    "    print(\"input features are\", Features)\n",
    "    #  we make sure Features has the right shape (list-of-lists)\n",
    "    row = np.array( [Features] )  # makes an array-row\n",
    "    row = scaler.transform(row)   # scale according to scaler\n",
    "    print(\"nn.predict_proba == \", nn.predict_proba(row))   # probabilities of each\n",
    "    prediction = nn.predict(row)  # max!\n",
    "    return prediction\n",
    "    \n",
    "# our features -- note that the inputs don't have to be bits!\n",
    "Features = [ 1, 0, 1 ]      # whatever we'd like to test - need not be binary at all!\n",
    "prediction = make_prediction(Features, nn_classifier, scaler)\n",
    "print(f\"prediction: {prediction}\")   # just takes the max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "++++++++++  TRAINING:  begin  +++++++++++++++\n",
      "\n",
      "\n",
      "Iteration 1, loss = 0.40816949\n",
      "Iteration 2, loss = 0.22557962\n",
      "Iteration 3, loss = 0.15897929\n",
      "Iteration 4, loss = 0.14989194\n",
      "Iteration 5, loss = 0.15522071\n",
      "Iteration 6, loss = 0.15797370\n",
      "Iteration 7, loss = 0.15308857\n",
      "Iteration 8, loss = 0.14175494\n",
      "Iteration 9, loss = 0.12827512\n",
      "Iteration 10, loss = 0.11664322\n",
      "Iteration 11, loss = 0.10867651\n",
      "Iteration 12, loss = 0.10417694\n",
      "Iteration 13, loss = 0.10193327\n",
      "Iteration 14, loss = 0.10056177\n",
      "Iteration 15, loss = 0.09895995\n",
      "Iteration 16, loss = 0.09648523\n",
      "Iteration 17, loss = 0.09295917\n",
      "Iteration 18, loss = 0.08855738\n",
      "Iteration 19, loss = 0.08364507\n",
      "Iteration 20, loss = 0.07861295\n",
      "Iteration 21, loss = 0.07374884\n",
      "Iteration 22, loss = 0.06916507\n",
      "Iteration 23, loss = 0.06479600\n",
      "Iteration 24, loss = 0.06046830\n",
      "Iteration 25, loss = 0.05601735\n",
      "Iteration 26, loss = 0.05139166\n",
      "Iteration 27, loss = 0.04668579\n",
      "Iteration 28, loss = 0.04208702\n",
      "Iteration 29, loss = 0.03777911\n",
      "Iteration 30, loss = 0.03386973\n",
      "Iteration 31, loss = 0.03037974\n",
      "Iteration 32, loss = 0.02728211\n",
      "Iteration 33, loss = 0.02454835\n",
      "Iteration 34, loss = 0.02216837\n",
      "Iteration 35, loss = 0.02014008\n",
      "Iteration 36, loss = 0.01844801\n",
      "Iteration 37, loss = 0.01704976\n",
      "Iteration 38, loss = 0.01587555\n",
      "Iteration 39, loss = 0.01483685\n",
      "Iteration 40, loss = 0.01384009\n",
      "Iteration 41, loss = 0.01280449\n",
      "Iteration 42, loss = 0.01168051\n",
      "Iteration 43, loss = 0.01046142\n",
      "Iteration 44, loss = 0.00918083\n",
      "Iteration 45, loss = 0.00789628\n",
      "Iteration 46, loss = 0.00666679\n",
      "Iteration 47, loss = 0.00553505\n",
      "Iteration 48, loss = 0.00452088\n",
      "Iteration 49, loss = 0.00362579\n",
      "Iteration 50, loss = 0.00284303\n",
      "Iteration 51, loss = 0.00216639\n",
      "Iteration 52, loss = 0.00159401\n",
      "Iteration 53, loss = 0.00112679\n",
      "Iteration 54, loss = 0.00076420\n",
      "Iteration 55, loss = 0.00050073\n",
      "Iteration 56, loss = 0.00032486\n",
      "Iteration 57, loss = 0.00022072\n",
      "Iteration 58, loss = 0.00017102\n",
      "Iteration 59, loss = 0.00015972\n",
      "Iteration 60, loss = 0.00017355\n",
      "Iteration 61, loss = 0.00020233\n",
      "Iteration 62, loss = 0.00023840\n",
      "Iteration 63, loss = 0.00027608\n",
      "Iteration 64, loss = 0.00031118\n",
      "Iteration 65, loss = 0.00034081\n",
      "Iteration 66, loss = 0.00036326\n",
      "Iteration 67, loss = 0.00037789\n",
      "Iteration 68, loss = 0.00038487\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.020000\n",
      "Iteration 69, loss = 0.00038491\n",
      "Iteration 70, loss = 0.00039140\n",
      "Iteration 71, loss = 0.00039703\n",
      "Iteration 72, loss = 0.00040160\n",
      "Iteration 73, loss = 0.00040496\n",
      "Iteration 74, loss = 0.00040701\n",
      "Iteration 75, loss = 0.00040774\n",
      "Iteration 76, loss = 0.00040715\n",
      "Iteration 77, loss = 0.00040526\n",
      "Iteration 78, loss = 0.00040216\n",
      "Iteration 79, loss = 0.00039793\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.004000\n",
      "Iteration 80, loss = 0.00039267\n",
      "Iteration 81, loss = 0.00038897\n",
      "Iteration 82, loss = 0.00038544\n",
      "Iteration 83, loss = 0.00038205\n",
      "Iteration 84, loss = 0.00037877\n",
      "Iteration 85, loss = 0.00037558\n",
      "Iteration 86, loss = 0.00037245\n",
      "Iteration 87, loss = 0.00036937\n",
      "Iteration 88, loss = 0.00036634\n",
      "Iteration 89, loss = 0.00036334\n",
      "Iteration 90, loss = 0.00036037\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000800\n",
      "Iteration 91, loss = 0.00035743\n",
      "Iteration 92, loss = 0.00035496\n",
      "Iteration 93, loss = 0.00035270\n",
      "Iteration 94, loss = 0.00035062\n",
      "Iteration 95, loss = 0.00034870\n",
      "Iteration 96, loss = 0.00034693\n",
      "Iteration 97, loss = 0.00034529\n",
      "Iteration 98, loss = 0.00034376\n",
      "Iteration 99, loss = 0.00034234\n",
      "Iteration 100, loss = 0.00034101\n",
      "Iteration 101, loss = 0.00033976\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000160\n",
      "Iteration 102, loss = 0.00033858\n",
      "Iteration 103, loss = 0.00033755\n",
      "Iteration 104, loss = 0.00033662\n",
      "Iteration 105, loss = 0.00033577\n",
      "Iteration 106, loss = 0.00033500\n",
      "Iteration 107, loss = 0.00033429\n",
      "Iteration 108, loss = 0.00033365\n",
      "Iteration 109, loss = 0.00033306\n",
      "Iteration 110, loss = 0.00033252\n",
      "Iteration 111, loss = 0.00033202\n",
      "Iteration 112, loss = 0.00033157\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000032\n",
      "Iteration 113, loss = 0.00033114\n",
      "Iteration 114, loss = 0.00033077\n",
      "Iteration 115, loss = 0.00033043\n",
      "Iteration 116, loss = 0.00033013\n",
      "Iteration 117, loss = 0.00032985\n",
      "Iteration 118, loss = 0.00032960\n",
      "Iteration 119, loss = 0.00032937\n",
      "Iteration 120, loss = 0.00032916\n",
      "Iteration 121, loss = 0.00032897\n",
      "Iteration 122, loss = 0.00032880\n",
      "Iteration 123, loss = 0.00032864\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000006\n",
      "Iteration 124, loss = 0.00032850\n",
      "Iteration 125, loss = 0.00032837\n",
      "Iteration 126, loss = 0.00032826\n",
      "Iteration 127, loss = 0.00032815\n",
      "Iteration 128, loss = 0.00032806\n",
      "Iteration 129, loss = 0.00032797\n",
      "Iteration 130, loss = 0.00032789\n",
      "Iteration 131, loss = 0.00032783\n",
      "Iteration 132, loss = 0.00032776\n",
      "Iteration 133, loss = 0.00032770\n",
      "Iteration 134, loss = 0.00032765\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000001\n",
      "Iteration 135, loss = 0.00032761\n",
      "Iteration 136, loss = 0.00032756\n",
      "Iteration 137, loss = 0.00032752\n",
      "Iteration 138, loss = 0.00032749\n",
      "Iteration 139, loss = 0.00032746\n",
      "Iteration 140, loss = 0.00032743\n",
      "Iteration 141, loss = 0.00032741\n",
      "Iteration 142, loss = 0.00032738\n",
      "Iteration 143, loss = 0.00032736\n",
      "Iteration 144, loss = 0.00032734\n",
      "Iteration 145, loss = 0.00032733\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
      "Iteration 146, loss = 0.00032731\n",
      "Iteration 147, loss = 0.00032730\n",
      "Iteration 148, loss = 0.00032729\n",
      "Iteration 149, loss = 0.00032727\n",
      "Iteration 150, loss = 0.00032726\n",
      "Iteration 151, loss = 0.00032726\n",
      "Iteration 152, loss = 0.00032725\n",
      "Iteration 153, loss = 0.00032724\n",
      "Iteration 154, loss = 0.00032723\n",
      "Iteration 155, loss = 0.00032723\n",
      "Iteration 156, loss = 0.00032722\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
      "++++++++++  TRAINING:   end  +++++++++++++++\n",
      "The (squared) prediction error (the loss) is 0.0003272216339155271\n",
      "And, its square root: 0.018089268473753358\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# MLPRegressor predicts _floating-point_ outputs\n",
    "#\n",
    "\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "nn_regressor = MLPRegressor(hidden_layer_sizes=(6,7), \n",
    "                    max_iter=200,          # how many training epochs\n",
    "                    activation=\"tanh\",     # the activation function\n",
    "                    solver='sgd',          # the optimizer\n",
    "                    verbose=True,          # do we want to watch as it trains?\n",
    "                    shuffle=True,          # shuffle each epoch?\n",
    "                    random_state=None,     # use for reproducibility\n",
    "                    learning_rate_init=.1, # how much of each error to back-propagate\n",
    "                    learning_rate = 'adaptive')  # how to handle the learning_rate\n",
    "\n",
    "print(\"\\n\\n++++++++++  TRAINING:  begin  +++++++++++++++\\n\\n\")\n",
    "nn_regressor.fit(X_train_scaled, y_train_scaled)\n",
    "print(\"++++++++++  TRAINING:   end  +++++++++++++++\")\n",
    "\n",
    "print(f\"The (squared) prediction error (the loss) is {nn_regressor.loss_}\")\n",
    "print(f\"And, its square root: {nn_regressor.loss_ ** 0.5}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            input  ->   pred    desr    absdiff  \n",
      "        [0. 0. 0.] ->  -0.033  +0.000    0.033   \n",
      "        [0. 0. 1.] ->  +1.044  +1.000    0.044   \n",
      "        [0. 1. 0.] ->  +1.013  +1.000    0.013   \n",
      "        [0. 1. 1.] ->  -0.008  +0.000    0.008   \n",
      "        [1. 0. 0.] ->  +1.004  +1.000    0.004   \n",
      "        [1. 0. 1.] ->  -0.004  +0.000    0.004   \n",
      "        [1. 1. 0.] ->  +0.005  +0.000    0.005   \n",
      "        [1. 1. 1.] ->  +0.985  +1.000    0.015   \n",
      "\n",
      "average absolute error: 0.015785068630739894\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# how did it do? We're making progress ... by regressing:\n",
    "#\n",
    "def ascii_table_for_regressor(Xsc,y,nn,scaler):\n",
    "    \"\"\" a table including predictions using nn.predict \"\"\"\n",
    "    predictions = nn.predict(Xsc) # all predictions\n",
    "    Xpr = scaler.inverse_transform(Xsc)  # Xpr is the \"X to print\": unscaled data!\n",
    "    # measure error\n",
    "    total_error = 0.0\n",
    "    # printing\n",
    "    print(f\"{'input ':>18s} ->  {'pred':^6s}  {'desr':^6s}  {'absdiff':^10s}\") \n",
    "    for i in range(len(y)):\n",
    "        pred = predictions[i]\n",
    "        desired = y[i]\n",
    "        result = abs(desired - pred)\n",
    "        total_error += result   # add up the errors\n",
    "        # Xpr = Xsc   # if you'd like to see the scaled values\n",
    "        print(f\"{Xpr[i,:]!s:>18s} ->  {pred:<+6.3f}  {desired:<+6.3f}  {result:^10.3f}\") \n",
    "    print(f\"\\naverage absolute error: {total_error/len(y)}\")\n",
    "    \n",
    "#\n",
    "# let's see how it did on the test data (also the training data!)\n",
    "#\n",
    "ascii_table_for_regressor(X_test_scaled,\n",
    "                          y_test_scaled,\n",
    "                          nn_regressor,\n",
    "                          scaler)   # this is our own f'n, above\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "+++++ parameters, weights, etc. +++++\n",
      "\n",
      "\n",
      "weights/coefficients:\n",
      "\n",
      "[[ 0.67234027 -0.15541633 -0.48714159 -0.29596586 -0.6629438   0.84078248]\n",
      " [ 0.79441176 -0.00594859 -0.23616109  0.30008869 -0.94297981  0.72750852]\n",
      " [-0.65395898  0.63055687  0.18673944 -0.48559702  0.77141119  0.20476742]]\n",
      "[[ 5.72456371e-01 -2.45249926e-01 -8.76576481e-01  6.88029105e-02\n",
      "  -8.71301179e-02  2.09262955e-01 -7.53743516e-02]\n",
      " [ 4.83404322e-01  1.51198668e-01  9.27433727e-01  5.16873045e-01\n",
      "  -1.99173435e-01  3.20704077e-04 -1.72875316e-01]\n",
      " [ 7.06327326e-02  3.26529000e-02  3.59277292e-01  2.84301763e-03\n",
      "  -3.87523910e-01  1.40775314e-01 -8.38017819e-01]\n",
      " [-6.88574265e-01  1.05831474e-01  9.69980318e-02 -4.59445920e-01\n",
      "  -5.88829726e-01 -5.74059078e-01  2.05473072e-01]\n",
      " [ 2.93135316e-01  1.48655930e-01  9.61826462e-01 -4.71280819e-01\n",
      "   3.87037866e-02  1.17475796e-01 -1.18068391e-01]\n",
      " [ 6.48279326e-01 -8.10026091e-01 -4.10578364e-01 -6.26260476e-01\n",
      "   2.16497832e-01 -3.66611124e-01 -1.10505814e-02]]\n",
      "[[ 0.3234286 ]\n",
      " [ 0.60665275]\n",
      " [-0.79017526]\n",
      " [ 1.00924537]\n",
      " [-0.07361333]\n",
      " [ 0.26884789]\n",
      " [-0.61579154]]\n",
      "\n",
      "intercepts: [array([ 0.0492758 ,  0.41204986,  0.71984807, -0.07778975,  0.15353294,\n",
      "        0.02750519]), array([ 0.65837806, -0.15081486, -0.07447296, -0.09179147, -0.08962175,\n",
      "       -0.70942054, -0.49066531]), array([0.06247441])]\n",
      "\n",
      "all parameters: {'activation': 'tanh', 'alpha': 0.0001, 'batch_size': 'auto', 'beta_1': 0.9, 'beta_2': 0.999, 'early_stopping': False, 'epsilon': 1e-08, 'hidden_layer_sizes': (6, 7), 'learning_rate': 'adaptive', 'learning_rate_init': 0.1, 'max_fun': 15000, 'max_iter': 200, 'momentum': 0.9, 'n_iter_no_change': 10, 'nesterovs_momentum': True, 'power_t': 0.5, 'random_state': None, 'shuffle': True, 'solver': 'sgd', 'tol': 0.0001, 'validation_fraction': 0.1, 'verbose': True, 'warm_start': False}\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# other details we might want to see for the regressor... (less often)\n",
    "#\n",
    "nn = nn_regressor  # less to type?\n",
    "print(\"\\n\\n+++++ parameters, weights, etc. +++++\\n\")\n",
    "print(f\"\\nweights/coefficients:\\n\")\n",
    "for wts in nn.coefs_:\n",
    "    print(wts)\n",
    "print(f\"\\nintercepts: {nn.intercepts_}\")\n",
    "print(f\"\\nall parameters: {nn.get_params()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input features are [1.0, 0.0, 1.0]\n",
      "prediction: [-0.0041837]\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# again, we have a predictive model. This time, it's a regressor.  Let's try it out...\n",
    "#\n",
    "\n",
    "def make_prediction( Features, nn, scaler ):\n",
    "    \"\"\" use a NNet regressor to make a prediction \"\"\"\n",
    "    print(\"input features are\", Features)\n",
    "    row = np.array( [Features] )  # a list-of-lists-style input is needed\n",
    "    row = scaler.transform(row)   # scale!\n",
    "    prediction = nn.predict(row)\n",
    "    #print(\"nn.predict yields \", prediction)\n",
    "    return prediction\n",
    "    \n",
    "# our features\n",
    "Features = [ 1.0, 0.0, 1.0 ]     # can vary these to be anything!\n",
    "prediction = make_prediction(Features, nn_regressor, scaler)\n",
    "print(f\"prediction: {prediction}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "1f0c8735478a48ff7ef3deb8c421f15aa5d573c59a98bc92eb9b829f28c47b33"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
